{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNyK7zRIcGbaDeVwErAh7m7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VadimDu/Protein_LLM_modeling/blob/main/Modeling_ESM_plusplus.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modeling Synthyra/ESMplusplus_large protein LLM\n",
        "ESM-plus-plus is a more effienct and Huggingface-compatibile implementation of the new ESM3 (ESM-C)LLM for protein embeddings.\n",
        "\n",
        "The cells below will be implemented via HuggingFace libraries and advanced wrapper scripts of the latest ESM models, inc. the newly relased ESM3 (ESM-C).\n",
        "\n",
        "The code and models below are obtained from [HuggingFace/Synthyra/ESMplusplus_large](https://huggingface.co/Synthyra/ESMplusplus_large) repo, which corresponds to the large version of 600 million parameter ESM-C model.\n",
        "\n",
        "ESM++ is a faithful implementation of ESMC (license) that allows for batching and standard Huggingface compatibility without requiring the ESM Python package.\n",
        "\n",
        "According to the authors this 600M parameter ESM-C model rivals the 3B parameter ESM2 and approaches the capabilities of the 15B model, delivering frontier performance with far greater efficiency (in terms of computational speed  & resources)."
      ],
      "metadata": {
        "id": "N0ig7KP8Fm8W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below is obtain from [modeling_esm_plusplus.py](https://huggingface.co/Synthyra/ESMplusplus_large/blob/main/modeling_esm_plusplus.py) that suppose to be a general code for various downstream supervised classification tasks."
      ],
      "metadata": {
        "id": "6o5ZEmbLGrB4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports and env. variables"
      ],
      "metadata": {
        "id": "B0xCdz9pHKOK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "ESM++ model implementation.\n",
        "\n",
        "ESM++ is a faithful implementation of ESMC that allows for batching and standard Huggingface compatibility\n",
        "The ESM Python package is not required\n",
        "\n",
        "Modified from https://github.com/evolutionaryscale/esm\n",
        "License: https://www.evolutionaryscale.ai/policies/cambrian-non-commercial-license-agreement\n",
        "\"\"\"\n",
        "\n",
        "import math\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from dataclasses import dataclass\n",
        "from functools import cache, partial\n",
        "from pathlib import Path\n",
        "from typing import Optional, Tuple, Union\n",
        "from einops import rearrange, repeat\n",
        "from huggingface_hub import snapshot_download\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.processors import TemplateProcessing\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import PreTrainedModel, PreTrainedTokenizerFast, PretrainedConfig\n",
        "from transformers.modeling_outputs import ModelOutput"
      ],
      "metadata": {
        "id": "u7ppcQk3GqNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuration class for ESM++ model, multi-head with rotary position embeddings, transformer & ffn layers, output types"
      ],
      "metadata": {
        "id": "tXE5ljC9Djxn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ESMplusplusConfig(PretrainedConfig):\n",
        "    \"\"\"Configuration class for ESM++ model.\n",
        "\n",
        "    Args:\n",
        "        vocab_size: Size of the vocabulary\n",
        "        hidden_size: Dimension of hidden layers\n",
        "        num_attention_heads: Number of attention heads\n",
        "        num_hidden_layers: Number of transformer layers\n",
        "        num_labels: Number of output labels for classification\n",
        "        problem_type: Type of problem - regression, single/multi label classification\n",
        "    \"\"\"\n",
        "    model_type = \"ESMplusplus\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int = 64,\n",
        "        hidden_size: int = 960,\n",
        "        num_attention_heads: int = 15,\n",
        "        num_hidden_layers: int = 30,\n",
        "        num_labels: int = 2,\n",
        "        problem_type: str | None = None,\n",
        "        dropout: float = 0.0,\n",
        "        initializer_range: float = 0.02,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_attention_heads = num_attention_heads\n",
        "        self.num_hidden_layers = num_hidden_layers\n",
        "        self.num_labels = num_labels\n",
        "        self.problem_type = problem_type\n",
        "        self.dropout = dropout\n",
        "        self.initializer_range = initializer_range\n",
        "\n",
        "\n",
        "### Rotary Embeddings\n",
        "def rotate_half(x: torch.Tensor, interleaved: bool = False) -> torch.Tensor:\n",
        "    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
        "    if not interleaved:\n",
        "        x1, x2 = x.chunk(2, dim=-1)\n",
        "        return torch.cat((-x2, x1), dim=-1)\n",
        "    else:\n",
        "        x1, x2 = x[..., ::2], x[..., 1::2]\n",
        "        return rearrange(\n",
        "            torch.stack((-x2, x1), dim=-1), \"... d two -> ... (d two)\", two=2\n",
        "        )\n",
        "\n",
        "\n",
        "def apply_rotary_emb_torch(\n",
        "    x: torch.Tensor,\n",
        "    cos: torch.Tensor,\n",
        "    sin: torch.Tensor,\n",
        "    interleaved: bool = False,\n",
        "    _inplace: bool = False,\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"Apply rotary embeddings to input based on cos and sin.\"\"\"\n",
        "    ro_dim = cos.shape[-1] * 2\n",
        "    assert ro_dim <= x.shape[-1]\n",
        "    seqlen = x.size(1)\n",
        "    cos = cos[:seqlen]\n",
        "    sin = sin[:seqlen]\n",
        "    cos = repeat(cos, \"s d -> s 1 (2 d)\")\n",
        "    sin = repeat(sin, \"s d -> s 1 (2 d)\")\n",
        "    return torch.cat(\n",
        "        [\n",
        "            x[..., :ro_dim] * cos + rotate_half(x[..., :ro_dim], interleaved) * sin,\n",
        "            x[..., ro_dim:],\n",
        "        ],\n",
        "        dim=-1,\n",
        "    )\n",
        "\n",
        "\n",
        "class RotaryEmbedding(torch.nn.Module):\n",
        "    \"\"\"Rotary position embeddings.\n",
        "\n",
        "    Based on the paper \"RoFormer: Enhanced Transformer with Rotary Position Embedding\"\n",
        "\n",
        "    Args:\n",
        "        dim: Dimension of the embedding\n",
        "        base: Base for computing angular frequencies\n",
        "        interleaved: Whether to use interleaved rotations\n",
        "        scale_base: Base for scaling\n",
        "        scaling_factor: Factor for scaling positions\n",
        "        pos_idx_in_fp32: Whether to compute position indices in fp32\n",
        "        device: Computation device\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int,\n",
        "        base: float = 10000.0,\n",
        "        interleaved: bool = False,\n",
        "        scale_base: Optional[float] = None,\n",
        "        scaling_factor: float = 1.0,\n",
        "        pos_idx_in_fp32: bool = True,\n",
        "        device: Optional[torch.device] = None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.base = float(base)\n",
        "        self.pos_idx_in_fp32 = pos_idx_in_fp32\n",
        "        self.interleaved = interleaved\n",
        "        self.scale_base = scale_base\n",
        "        self.scaling_factor = scaling_factor\n",
        "        self.device = device\n",
        "\n",
        "        self._seq_len_cached = 0\n",
        "        self._cos_cached = None\n",
        "        self._sin_cached = None\n",
        "        self._cos_k_cached = None\n",
        "        self._sin_k_cached = None\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        \"\"\"Reset the parameters of the embedding.\"\"\"\n",
        "        inv_freq = self._compute_inv_freq(self.device)\n",
        "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
        "        arange = torch.arange(0, self.dim, 2, device=self.device, dtype=torch.float32)\n",
        "        scale = (\n",
        "            (arange + 0.4 * self.dim) / (1.4 * self.dim)\n",
        "            if self.scale_base is not None\n",
        "            else None\n",
        "        )\n",
        "        self.register_buffer(\"scale\", scale)\n",
        "\n",
        "    def _compute_inv_freq(self, device: Optional[torch.device] = None) -> torch.Tensor:\n",
        "        \"\"\"Compute inverse frequency bands.\"\"\"\n",
        "        return 1 / (\n",
        "            self.base\n",
        "            ** (\n",
        "                torch.arange(0, self.dim, 2, device=device, dtype=torch.float32)\n",
        "                / self.dim\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def _update_cos_sin_cache(self, seqlen: int, device: Optional[torch.device] = None, dtype: Optional[torch.dtype] = None):\n",
        "        \"\"\"Update the cached cosine and sine values.\"\"\"\n",
        "        if (\n",
        "            seqlen > self._seq_len_cached\n",
        "            or self._cos_cached is None\n",
        "            or self._cos_cached.device != device\n",
        "            or self._cos_cached.dtype != dtype\n",
        "            or (self.training and self._cos_cached.is_inference())\n",
        "        ):\n",
        "            self._seq_len_cached = seqlen\n",
        "            if self.pos_idx_in_fp32:\n",
        "                t = torch.arange(seqlen, device=device, dtype=torch.float32)\n",
        "                t /= self.scaling_factor\n",
        "                if self.inv_freq.dtype != torch.float32:\n",
        "                    inv_freq = self.inv_freq.to(torch.float32)\n",
        "                else:\n",
        "                    inv_freq = self.inv_freq\n",
        "            else:\n",
        "                t = torch.arange(seqlen, device=device, dtype=self.inv_freq.dtype)\n",
        "                t /= self.scaling_factor\n",
        "                inv_freq = self.inv_freq\n",
        "            freqs = torch.outer(t, inv_freq)\n",
        "\n",
        "            if self.scale is None:\n",
        "                self._cos_cached = torch.cos(freqs).to(dtype)\n",
        "                self._sin_cached = torch.sin(freqs).to(dtype)\n",
        "            else:\n",
        "                power = (\n",
        "                    torch.arange(\n",
        "                        seqlen, dtype=self.scale.dtype, device=self.scale.device\n",
        "                    )\n",
        "                    - seqlen // 2\n",
        "                ) / self.scale_base\n",
        "                scale = self.scale.to(device=power.device) ** power.unsqueeze(-1)\n",
        "                self._cos_cached = (torch.cos(freqs) * scale).to(dtype)\n",
        "                self._sin_cached = (torch.sin(freqs) * scale).to(dtype)\n",
        "                self._cos_k_cached = (torch.cos(freqs) / scale).to(dtype)\n",
        "                self._sin_k_cached = (torch.sin(freqs) / scale).to(dtype)\n",
        "\n",
        "    def forward(self, q: torch.Tensor, k: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Apply rotary embeddings to queries and keys.\n",
        "\n",
        "        Args:\n",
        "            q: Query tensor of shape (batch, seqlen, nheads, headdim)\n",
        "            k: Key tensor of shape (batch, seqlen, nheads, headdim)\n",
        "\n",
        "        Returns:\n",
        "            Tuple of rotated query and key tensors\n",
        "        \"\"\"\n",
        "        self._update_cos_sin_cache(q.shape[1], device=q.device, dtype=q.dtype)\n",
        "        assert self._cos_cached is not None\n",
        "        assert self._sin_cached is not None\n",
        "        if self.scale is None:\n",
        "            return (\n",
        "                apply_rotary_emb_torch(\n",
        "                    q,\n",
        "                    self._cos_cached,\n",
        "                    self._sin_cached,\n",
        "                    self.interleaved,\n",
        "                    True,  # inplace=True\n",
        "                ),\n",
        "                apply_rotary_emb_torch(\n",
        "                    k,\n",
        "                    self._cos_cached,\n",
        "                    self._sin_cached,\n",
        "                    self.interleaved,\n",
        "                    True,  # inplace=True\n",
        "                ),\n",
        "            )  # type: ignore\n",
        "        else:\n",
        "            assert False\n",
        "\n",
        "\n",
        "### Feedforward Network Components\n",
        "def swiglu_correction_fn(expansion_ratio: float, d_model: int) -> int:\n",
        "    \"\"\"Compute corrected dimension for SwiGLU.\"\"\"\n",
        "    return int(((expansion_ratio * d_model) + 255) // 256 * 256)\n",
        "\n",
        "\n",
        "class SwiGLU(nn.Module):\n",
        "    \"\"\"SwiGLU activation function.\"\"\"\n",
        "    def __init__(self):\n",
        "        super(SwiGLU, self).__init__()\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x1, x2 = x.chunk(2, dim=-1)\n",
        "        return F.silu(x1) * x2\n",
        "\n",
        "\n",
        "def swiglu_ln_ffn(d_model: int, expansion_ratio: float) -> nn.Sequential:\n",
        "    \"\"\"Create SwiGLU feedforward network with layer normalization.\"\"\"\n",
        "    return nn.Sequential(\n",
        "        nn.LayerNorm(d_model),\n",
        "        nn.Linear(\n",
        "            d_model, swiglu_correction_fn(expansion_ratio, d_model) * 2, bias=False\n",
        "        ),\n",
        "        SwiGLU(),\n",
        "        nn.Linear(swiglu_correction_fn(expansion_ratio, d_model), d_model, bias=False),\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "### Transformer block with attention and feedforward layers\n",
        "class UnifiedTransformerBlock(nn.Module):\n",
        "    \"\"\"Transformer block with attention and feedforward layers.\n",
        "\n",
        "    Args:\n",
        "        d_model: Model dimension\n",
        "        n_heads: Number of attention heads\n",
        "        residue_scaling_factor: Factor for scaling residual connections\n",
        "        expansion_ratio: Expansion ratio for feedforward network\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        d_model: int,\n",
        "        n_heads: int,\n",
        "        residue_scaling_factor: float = 1,\n",
        "        expansion_ratio: float = 8 / 3,\n",
        "        dropout: float = 0.0,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.attn = MultiHeadAttention(d_model, n_heads)\n",
        "        self.ffn = swiglu_ln_ffn(d_model, expansion_ratio)\n",
        "        self.scaling_factor = residue_scaling_factor\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x: torch.Tensor,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        output_attentions: bool = False,\n",
        "    ) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input tensor\n",
        "            attention_mask: Optional attention mask\n",
        "            output_attentions: Whether to return attention weights\n",
        "\n",
        "        Returns:\n",
        "            Output tensor after transformer block, and optionally attention weights\n",
        "        \"\"\"\n",
        "        attn_output, attn_weights = self.attn(x, attention_mask, output_attentions)\n",
        "        x = x + self.dropout(attn_output) / self.scaling_factor\n",
        "        x = x + self.dropout(self.ffn(x)) / self.scaling_factor\n",
        "        return x, attn_weights\n",
        "\n",
        "\n",
        "### Multi-head attention with rotary embeddings\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"Multi-head attention with rotary embeddings.\n",
        "\n",
        "    Args:\n",
        "        d_model: Model dimension\n",
        "        n_heads: Number of attention heads\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model: int, n_heads: int):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.d_head = self.d_model // self.n_heads\n",
        "        self.layernorm_qkv = nn.Sequential(\n",
        "            nn.LayerNorm(d_model), nn.Linear(d_model, d_model * 3, bias=False)\n",
        "        )\n",
        "        self.out_proj = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.q_ln = nn.LayerNorm(d_model, bias=False)\n",
        "        self.k_ln = nn.LayerNorm(d_model, bias=False)\n",
        "        self.reshaper = partial(rearrange, pattern=\"b s (h d) -> b h s d\", h=n_heads)\n",
        "        self.rotary = RotaryEmbedding(d_model // n_heads)\n",
        "\n",
        "    def _apply_rotary(self, q: torch.Tensor, k: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Apply rotary embeddings to query and key.\"\"\"\n",
        "        q = q.unflatten(-1, (self.n_heads, self.d_head))\n",
        "        k = k.unflatten(-1, (self.n_heads, self.d_head))\n",
        "        q, k = self.rotary(q, k)\n",
        "        q = q.flatten(-2, -1)\n",
        "        k = k.flatten(-2, -1)\n",
        "        return q, k\n",
        "\n",
        "    def forward(self, x: torch.Tensor, attention_mask: Optional[torch.Tensor] = None, output_attentions: bool = False) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input tensor\n",
        "            attention_mask: Optional attention mask\n",
        "            output_attentions: Whether to return attention weights\n",
        "\n",
        "        Returns:\n",
        "            Output tensor after self attention, and optionally attention weights\n",
        "        \"\"\"\n",
        "        attn_weights = None\n",
        "        qkv_BLD3 = self.layernorm_qkv(x)\n",
        "        query_BLD, key_BLD, value_BLD = torch.chunk(qkv_BLD3, 3, dim=-1)\n",
        "        query_BLD, key_BLD = (\n",
        "            self.q_ln(query_BLD).to(query_BLD.dtype),\n",
        "            self.k_ln(key_BLD).to(query_BLD.dtype),\n",
        "        )\n",
        "        query_BLD, key_BLD = self._apply_rotary(query_BLD, key_BLD)\n",
        "        query_BHLD, key_BHLD, value_BHLD = map(self.reshaper, (query_BLD, key_BLD, value_BLD))\n",
        "\n",
        "        if output_attentions: # Manual attention computation\n",
        "            L, S = query_BLD.size(-2), key_BLD.size(-2)\n",
        "            scale = 1 / math.sqrt(query_BLD.size(-1))\n",
        "            attn_bias = torch.zeros(L, S, dtype=query_BLD.dtype, device=query_BLD.device)\n",
        "            if attention_mask is not None:\n",
        "                if attention_mask.dtype == torch.bool:\n",
        "                    attention_mask.masked_fill_(attention_mask.logical_not(), float('-inf'))\n",
        "                else:\n",
        "                    attn_bias += attention_mask\n",
        "\n",
        "            attn_weights = torch.matmul(query_BHLD, key_BHLD.transpose(-2, -1)) * scale\n",
        "            attn_weights += attn_bias\n",
        "            attn_weights = F.softmax(attn_weights, dim=-1)\n",
        "            context_BHLD = torch.matmul(attn_weights, value_BHLD)\n",
        "        else:\n",
        "            context_BHLD = F.scaled_dot_product_attention(\n",
        "                query_BHLD, key_BHLD, value_BHLD, attention_mask\n",
        "            )\n",
        "\n",
        "        context_BLD = rearrange(context_BHLD, \"b h s d -> b s (h d)\")\n",
        "        output = self.out_proj(context_BLD)\n",
        "        return output, attn_weights\n",
        "\n",
        "\n",
        "### Regression Head\n",
        "def RegressionHead(d_model: int, output_dim: int, hidden_dim: Optional[int] = None) -> nn.Module:\n",
        "    \"\"\"Create a regression head with optional hidden dimension.\n",
        "\n",
        "    Args:\n",
        "        d_model: Input dimension\n",
        "        output_dim: Output dimension\n",
        "        hidden_dim: Optional hidden dimension (defaults to d_model)\n",
        "    \"\"\"\n",
        "    hidden_dim = hidden_dim if hidden_dim is not None else d_model\n",
        "    return nn.Sequential(\n",
        "        nn.Linear(d_model, hidden_dim),\n",
        "        nn.GELU(),\n",
        "        nn.LayerNorm(hidden_dim),\n",
        "        nn.Linear(hidden_dim, output_dim),\n",
        "    )\n",
        "\n",
        "\n",
        "### Model Outputs\n",
        "@dataclass\n",
        "class TransformerOutput(ModelOutput):\n",
        "    \"\"\"Output type for transformer encoder.\"\"\"\n",
        "    last_hidden_state: Optional[torch.Tensor] = None\n",
        "    hidden_states: Optional[Tuple[torch.Tensor]] = None\n",
        "    attentions: Optional[Tuple[torch.Tensor]] = None\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ESMplusplusOutput(ModelOutput):\n",
        "    \"\"\"Output type for ESM++ models.\"\"\"\n",
        "    loss: Optional[torch.Tensor] = None\n",
        "    logits: Optional[torch.Tensor] = None\n",
        "    last_hidden_state: Optional[torch.Tensor] = None\n",
        "    hidden_states: Optional[Tuple[torch.Tensor]] = None\n",
        "    attentions: Optional[Tuple[torch.Tensor]] = None\n",
        "\n",
        "\n",
        "### Transformer Stack\n",
        "class TransformerStack(nn.Module):\n",
        "    \"\"\"Stack of transformer blocks.\n",
        "\n",
        "    Args:\n",
        "        d_model: Model dimension\n",
        "        n_heads: Number of attention heads\n",
        "        n_layers: Number of transformer layers\n",
        "        dropout: Dropout rate\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        d_model: int,\n",
        "        n_heads: int,\n",
        "        n_layers: int,\n",
        "        dropout: float = 0.0,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.blocks = nn.ModuleList(\n",
        "            [\n",
        "                UnifiedTransformerBlock(\n",
        "                    d_model,\n",
        "                    n_heads,\n",
        "                    residue_scaling_factor=math.sqrt(n_layers / 36),\n",
        "                    dropout=dropout,\n",
        "                )\n",
        "                for i in range(n_layers)\n",
        "            ]\n",
        "        )\n",
        "        self.norm = nn.LayerNorm(d_model, bias=False)\n",
        "        self.gradient_checkpointing = False\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x: torch.Tensor,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        output_hidden_states: bool = False,\n",
        "        output_attentions: bool = False,\n",
        "    ) -> TransformerOutput:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input tensor\n",
        "            attention_mask: Optional attention mask\n",
        "            output_hidden_states: Whether to return all hidden states\n",
        "            output_attentions: Whether to return attention weights\n",
        "\n",
        "        Returns:\n",
        "            TransformerOutput containing last hidden state and optionally all hidden states and attention weights\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "        hidden_states = () if output_hidden_states else None\n",
        "        attentions = () if output_attentions else None\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            attention_mask = attention_mask[:, None, None, :].expand(batch_size, 1, seq_len, seq_len).bool()\n",
        "\n",
        "        for block in self.blocks:\n",
        "            if self.gradient_checkpointing and self.training:\n",
        "                x, attn_weights = self._gradient_checkpointing_func(\n",
        "                    block.__call__,\n",
        "                    x,\n",
        "                    attention_mask,\n",
        "                    output_attentions,\n",
        "                )\n",
        "            else:\n",
        "                x, attn_weights = block(x, attention_mask, output_attentions)\n",
        "\n",
        "            if attentions is not None:\n",
        "                attentions += (attn_weights,)\n",
        "\n",
        "            if output_hidden_states:\n",
        "                assert hidden_states is not None\n",
        "                hidden_states += (x,)\n",
        "\n",
        "        return TransformerOutput(\n",
        "            last_hidden_state=self.norm(x),\n",
        "            hidden_states=hidden_states,\n",
        "            attentions=attentions\n",
        "        )\n",
        "\n"
      ],
      "metadata": {
        "id": "yHM9hTRaBkuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset for Protein Sequences Embedding"
      ],
      "metadata": {
        "id": "s3zb9BftFar7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Dataset for Embedding\n",
        "class ProteinDataset(Dataset):\n",
        "    \"\"\"Simple dataset for protein sequences.\"\"\"\n",
        "    def __init__(self, sequences: list[str]):\n",
        "        self.sequences = sequences\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> str:\n",
        "        return self.sequences[idx]\n",
        "\n",
        "\n",
        "class PreTrainedESMplusplusModel(PreTrainedModel):\n",
        "    \"\"\"\n",
        "    init weights for ESM++ models\n",
        "    \"\"\"\n",
        "    config_class = ESMplusplusConfig\n",
        "    base_model_prefix = \"esm++\"\n",
        "    supports_gradient_checkpointing = True\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        \"\"\"Initialize the weights\"\"\"\n",
        "        if isinstance(module, nn.Linear):\n",
        "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "            if module.padding_idx is not None:\n",
        "                module.weight.data[module.padding_idx].zero_()\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "            module.weight.data.fill_(1.0)\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained_esm(cls, model_name: str):\n",
        "        \"\"\"Load a pretrained ESM++ model.\"\"\"\n",
        "        if '300' in model_name:\n",
        "            return ESMplusplus_300M()\n",
        "        elif '600' in model_name:\n",
        "            return ESMplusplus_600M()\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid model name: {model_name}\")\n",
        "\n",
        "    @property\n",
        "    def device(self) -> torch.device:\n",
        "        \"\"\"Get the device of the model.\"\"\"\n",
        "        return next(self.parameters()).device\n",
        "\n",
        "    def mean_pooling(self, x: torch.Tensor, attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
        "        \"\"\"Apply mean pooling to sequence outputs.\"\"\"\n",
        "        if attention_mask is None:\n",
        "            return x.mean(dim=1)\n",
        "        else:\n",
        "            attention_mask = attention_mask.unsqueeze(-1)\n",
        "            return (x * attention_mask).sum(dim=1) / attention_mask.sum(dim=1)\n",
        "\n",
        "    def max_pooling(self, x: torch.Tensor, attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
        "        \"\"\"Apply max pooling to sequence outputs.\"\"\"\n",
        "        if attention_mask is None:\n",
        "            return x.max(dim=1).values\n",
        "        else:\n",
        "            attention_mask = attention_mask.unsqueeze(-1)\n",
        "            return (x * attention_mask).max(dim=1).values\n",
        "\n",
        "    def cls_pooling(self, x: torch.Tensor, attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
        "        \"\"\"Apply cls pooling to sequence outputs.\"\"\"\n",
        "        return x[:, 0, :]\n",
        "\n",
        "    def _collate_fn(self, sequences: list[str]) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Collate function for batching sequences.\"\"\"\n",
        "        return self.tokenizer(sequences, return_tensors=\"pt\", padding='longest', pad_to_multiple_of=8)\n",
        "\n",
        "    def _read_sequences_from_db(self, db_path: str) -> set[str]:\n",
        "        \"\"\"Read sequences from SQLite database.\"\"\"\n",
        "        import sqlite3\n",
        "        sequences = []\n",
        "        with sqlite3.connect(db_path) as conn:\n",
        "            c = conn.cursor()\n",
        "            c.execute(\"SELECT sequence FROM embeddings\")\n",
        "            while True:\n",
        "                row = c.fetchone()\n",
        "                if row is None:\n",
        "                    break\n",
        "                sequences.append(row[0])\n",
        "        return set(sequences)\n",
        "\n",
        "    def embed_dataset(\n",
        "        self,\n",
        "        sequences: list[str],\n",
        "        batch_size: int = 2,\n",
        "        max_len: int = 512,\n",
        "        full_embeddings: bool = False,\n",
        "        full_precision: bool = False,\n",
        "        pooling_type: str = 'mean',\n",
        "        num_workers: int = 0,\n",
        "        sql: bool = False,\n",
        "        sql_db_path: str = 'embeddings.db',\n",
        "    ) -> Optional[dict[str, torch.Tensor]]:\n",
        "        \"\"\"Embed a dataset of protein sequences.\n",
        "\n",
        "        Args:\n",
        "            sequences: List of protein sequences\n",
        "            batch_size: Batch size for processing\n",
        "            max_len: Maximum sequence length\n",
        "            full_embeddings: Whether to return full residue-wise (True) embeddings or pooled (False)\n",
        "            full_precision: Whether to cast to full precision (float32) before storage - relevant for dict storage\n",
        "            pooling_type: Type of pooling ('mean' or 'cls')\n",
        "            num_workers: Number of workers for data loading, 0 for the main process\n",
        "            sql: Whether to store embeddings in SQLite database - will be stored in float32\n",
        "            sql_db_path: Path to SQLite database\n",
        "\n",
        "        Returns:\n",
        "            Dictionary mapping sequences to embeddings, or None if sql=True\n",
        "        \"\"\"\n",
        "        sequences = list(set([seq[:max_len] for seq in sequences]))\n",
        "        device = self.device\n",
        "\n",
        "        def get_embeddings(residue_embeddings: torch.Tensor, attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
        "            if full_embeddings:\n",
        "                return residue_embeddings\n",
        "            elif pooling_type == 'mean':\n",
        "                return self.mean_pooling(residue_embeddings, attention_mask)\n",
        "            elif pooling_type == 'max':\n",
        "                return self.max_pooling(residue_embeddings, attention_mask)\n",
        "            elif pooling_type == 'cls':\n",
        "                return self.cls_pooling(residue_embeddings, attention_mask)\n",
        "            else:\n",
        "                raise ValueError(f\"Invalid pooling type: {pooling_type}\")\n",
        "\n",
        "        sequences = list(set([seq[:max_len] for seq in sequences]))\n",
        "        if sql:\n",
        "            import sqlite3\n",
        "            conn = sqlite3.connect(sql_db_path)\n",
        "            c = conn.cursor()\n",
        "            c.execute('CREATE TABLE IF NOT EXISTS embeddings (sequence text PRIMARY KEY, embedding blob)')\n",
        "            already_embedded = self._read_sequences_from_db(sql_db_path)\n",
        "            to_embed = [seq for seq in sequences if seq not in already_embedded]\n",
        "            print(f\"Found {len(already_embedded)} already embedded sequences in {sql_db_path}\")\n",
        "            print(f\"Embedding {len(to_embed)} new sequences\")\n",
        "            if len(to_embed) > 0:\n",
        "                to_embed = sorted(to_embed, key=len, reverse=True)\n",
        "                dataset = ProteinDataset(to_embed)\n",
        "                dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=num_workers, collate_fn=self._collate_fn, shuffle=False)\n",
        "                with torch.no_grad():\n",
        "                    for i, batch in tqdm(enumerate(dataloader), total=len(dataloader), desc='Embedding batches'):\n",
        "                        seqs = to_embed[i * batch_size:(i + 1) * batch_size]\n",
        "                        input_ids, attention_mask = batch['input_ids'].to(device), batch['attention_mask'].to(device)\n",
        "                        x = self.embed(input_ids)\n",
        "                        residue_embeddings = self.transformer(x, attention_mask).last_hidden_state.detach().float() # required for sql\n",
        "                        embeddings = get_embeddings(residue_embeddings, attention_mask)\n",
        "\n",
        "                        for seq, emb, mask in zip(seqs, embeddings, attention_mask):\n",
        "                            if full_embeddings:\n",
        "                                emb = emb[mask.bool()]\n",
        "                            c.execute(\"INSERT OR REPLACE INTO embeddings VALUES (?, ?)\",\n",
        "                                    (seq, emb.cpu().numpy().tobytes()))\n",
        "\n",
        "                        if (i + 1) % 100 == 0:\n",
        "                            conn.commit()\n",
        "\n",
        "                conn.commit()\n",
        "            conn.close()\n",
        "            return None\n",
        "\n",
        "        embeddings_dict = {}\n",
        "        sequences = sorted(sequences, key=len, reverse=True)\n",
        "        dataset = ProteinDataset(sequences)\n",
        "        dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=num_workers, collate_fn=self._collate_fn, shuffle=False)\n",
        "        with torch.no_grad():\n",
        "            for i, batch in tqdm(enumerate(dataloader), total=len(dataloader), desc='Embedding batches'):\n",
        "                seqs = sequences[i * batch_size:(i + 1) * batch_size]\n",
        "                input_ids, attention_mask = batch['input_ids'].to(device), batch['attention_mask'].to(device)\n",
        "                x = self.embed(input_ids)\n",
        "                residue_embeddings = self.transformer(x, attention_mask).last_hidden_state.detach()\n",
        "                if full_precision:\n",
        "                    residue_embeddings = residue_embeddings.float()\n",
        "                embeddings = get_embeddings(residue_embeddings, attention_mask).cpu()\n",
        "                for seq, emb in zip(seqs, embeddings):\n",
        "                    embeddings_dict[seq] = emb\n",
        "\n",
        "        return embeddings_dict\n"
      ],
      "metadata": {
        "id": "cMMexauFFcQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ESM++ Models with masked language modeling head"
      ],
      "metadata": {
        "id": "mUtnu9OOGLTe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### ESM++ Models\n",
        "class ESMplusplusModel(PreTrainedESMplusplusModel):\n",
        "    \"\"\"\n",
        "    ESM++ model. transformer model with no heads\n",
        "    \"\"\"\n",
        "    config_class = ESMplusplusConfig\n",
        "    def __init__(self, config: ESMplusplusConfig, **kwargs):\n",
        "        super().__init__(config, **kwargs)\n",
        "        self.config = config\n",
        "        self.vocab_size = config.vocab_size\n",
        "        self.embed = nn.Embedding(self.vocab_size, config.hidden_size)\n",
        "        self.transformer = TransformerStack(config.hidden_size, config.num_attention_heads, config.num_hidden_layers, config.dropout)\n",
        "        self.tokenizer = EsmSequenceTokenizer()\n",
        "        self.init_weights()\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.embed\n",
        "\n",
        "    def set_input_embeddings(self, value):\n",
        "        self.embed = value\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: Optional[torch.Tensor] = None,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        inputs_embeds: Optional[torch.Tensor] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        return_dict: Optional[bool] = None, # to play nice with HF adjacent packages\n",
        "    ) -> TransformerOutput:\n",
        "        \"\"\"Forward pass for masked language modeling.\n",
        "\n",
        "        Args:\n",
        "            input_ids: Input token IDs\n",
        "            attention_mask: Attention mask\n",
        "            inputs_embeds: Optional precomputed embeddings\n",
        "            output_hidden_states: Whether to return all hidden states\n",
        "            output_attentions: Whether to return attention weights\n",
        "\n",
        "        Returns:\n",
        "            TransformerOutput containing last hidden state and optionally all hidden states and attention weights\n",
        "        \"\"\"\n",
        "        if inputs_embeds is None:\n",
        "            x = self.embed(input_ids)\n",
        "        else:\n",
        "            x = inputs_embeds\n",
        "        return self.transformer(x, attention_mask, output_hidden_states, output_attentions)\n",
        "\n",
        "\n",
        "class ESMplusplusForMaskedLM(PreTrainedESMplusplusModel):\n",
        "    \"\"\"\n",
        "    ESM++ model for masked language modeling.\n",
        "    Implements the base ESM++ architecture with a masked language modeling head.\n",
        "    \"\"\"\n",
        "    config_class = ESMplusplusConfig\n",
        "    def __init__(self, config: ESMplusplusConfig, **kwargs):\n",
        "        super().__init__(config, **kwargs)\n",
        "        self.config = config\n",
        "        self.vocab_size = config.vocab_size\n",
        "        self.embed = nn.Embedding(self.vocab_size, config.hidden_size)\n",
        "        self.transformer = TransformerStack(config.hidden_size, config.num_attention_heads, config.num_hidden_layers, config.dropout)\n",
        "        self.sequence_head = RegressionHead(config.hidden_size, self.vocab_size)\n",
        "        self.ce_loss = nn.CrossEntropyLoss()\n",
        "        self.tokenizer = EsmSequenceTokenizer()\n",
        "        self.init_weights()\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.embed\n",
        "\n",
        "    def set_input_embeddings(self, value):\n",
        "        self.embed = value\n",
        "\n",
        "    def get_output_embeddings(self):\n",
        "        return self.sequence_head[-1]\n",
        "\n",
        "    def set_output_embeddings(self, new_embeddings):\n",
        "        self.sequence_head[-1] = new_embeddings\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: Optional[torch.Tensor] = None,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        inputs_embeds: Optional[torch.Tensor] = None,\n",
        "        labels: Optional[torch.Tensor] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        return_dict: Optional[bool] = None, # to play nice with HF adjacent packages\n",
        "    ) -> ESMplusplusOutput:\n",
        "        \"\"\"Forward pass for masked language modeling.\n",
        "\n",
        "        Args:\n",
        "            input_ids: Input token IDs\n",
        "            attention_mask: Attention mask\n",
        "            inputs_embeds: Optional precomputed embeddings\n",
        "            labels: Optional labels for masked tokens\n",
        "            output_hidden_states: Whether to return all hidden states\n",
        "            output_attentions: Whether to return attention weights\n",
        "\n",
        "        Returns:\n",
        "            ESMplusplusOutput containing loss, logits, hidden states and attention weights\n",
        "        \"\"\"\n",
        "        if inputs_embeds is None:\n",
        "            x = self.embed(input_ids)\n",
        "        else:\n",
        "            x = inputs_embeds\n",
        "        output = self.transformer(x, attention_mask, output_hidden_states, output_attentions)\n",
        "        x = output.last_hidden_state\n",
        "        logits = self.sequence_head(x)\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss = self.ce_loss(logits.view(-1, self.vocab_size), labels.view(-1))\n",
        "        return ESMplusplusOutput(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            last_hidden_state=x,\n",
        "            hidden_states=output.hidden_states,\n",
        "            attentions=output.attentions,\n",
        "        )\n"
      ],
      "metadata": {
        "id": "G7lYIkzAGLfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ESM++ model for sequence and token classification\n",
        "Extends the base ESM++ model with a classification head for either sequence or token classification tasks."
      ],
      "metadata": {
        "id": "vqrL7mPCGdGP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ESMplusplusForSequenceClassification(ESMplusplusForMaskedLM):\n",
        "    \"\"\"\n",
        "    ESM++ model for sequence classification.\n",
        "    Extends the base ESM++ model with a classification head.\n",
        "    \"\"\"\n",
        "    def __init__(self, config: ESMplusplusConfig, **kwargs):\n",
        "        super().__init__(config, **kwargs)\n",
        "        self.config = config\n",
        "        self.num_labels = config.num_labels\n",
        "        self.classifier = RegressionHead(config.hidden_size * 2, config.num_labels, config.hidden_size * 4)\n",
        "        # Large intermediate projections help with sequence classification tasks (*4)\n",
        "        self.mse = nn.MSELoss()\n",
        "        self.ce = nn.CrossEntropyLoss()\n",
        "        self.bce = nn.BCEWithLogitsLoss()\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: Optional[torch.Tensor] = None,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        inputs_embeds: Optional[torch.Tensor] = None,\n",
        "        labels: Optional[torch.Tensor] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        return_dict: Optional[bool] = None, # to play nice with HF adjacent packages\n",
        "    ) -> ESMplusplusOutput:\n",
        "        \"\"\"Forward pass for sequence classification.\n",
        "\n",
        "        Args:\n",
        "            input_ids: Input token IDs\n",
        "            attention_mask: Attention mask\n",
        "            inputs_embeds: Optional precomputed embeddings\n",
        "            labels: Optional labels for classification\n",
        "            output_hidden_states: Whether to return all hidden states\n",
        "            output_attentions: Whether to return attention weights\n",
        "\n",
        "        Returns:\n",
        "            ESMplusplusOutput containing loss, logits, and hidden states\n",
        "        \"\"\"\n",
        "        output = super().forward(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            labels=None,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states\n",
        "        )\n",
        "        x = output.last_hidden_state\n",
        "        cls_features = x[:, 0, :]\n",
        "        mean_features = self.mean_pooling(x, attention_mask)\n",
        "        # we include mean pooling features to help with early convergence, the cost of this is basically zero\n",
        "        features = torch.cat([cls_features, mean_features], dim=-1)\n",
        "        logits = self.classifier(features)\n",
        "\n",
        "        # # Use this:\n",
        "        # pooled_embedding = self.mean_pooling(x, attention_mask)  # Calculate pooled embedding\n",
        "        # logits = self.classifier(pooled_embedding)  # Pass pooled embedding to classifier\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            labels = labels.to(logits.device)\n",
        "            if self.config.problem_type is None:\n",
        "                if self.num_labels == 1:\n",
        "                    self.config.problem_type = \"regression\"\n",
        "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
        "                    self.config.problem_type = \"single_label_classification\"\n",
        "                else:\n",
        "                    self.config.problem_type = \"multi_label_classification\"\n",
        "\n",
        "            if self.config.problem_type == \"regression\":\n",
        "                if self.num_labels == 1:\n",
        "                    loss = self.mse(logits.flatten(), labels.flatten())\n",
        "                else:\n",
        "                    loss = self.mse(logits, labels)\n",
        "            elif self.config.problem_type == \"single_label_classification\":\n",
        "                loss = self.ce(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "            elif self.config.problem_type == \"multi_label_classification\":\n",
        "                loss = self.bce(logits, labels)\n",
        "\n",
        "        return ESMplusplusOutput(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            last_hidden_state=x,\n",
        "            hidden_states=output.hidden_states,\n",
        "        )\n",
        "\n",
        "\n",
        "class ESMplusplusForTokenClassification(ESMplusplusForMaskedLM):\n",
        "    \"\"\"\n",
        "    ESM++ model for token classification.\n",
        "    Extends the base ESM++ model with a token classification head.\n",
        "    \"\"\"\n",
        "    def __init__(self, config: ESMplusplusConfig):\n",
        "        super().__init__(config)\n",
        "        self.config = config\n",
        "        self.num_labels = config.num_labels\n",
        "        self.classifier = RegressionHead(config.hidden_size, config.num_labels, config.hidden_size * 4)\n",
        "        # Large intermediate projections help with sequence classification tasks (*4)\n",
        "        self.loss_fct = nn.CrossEntropyLoss()\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: Optional[torch.Tensor] = None,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        inputs_embeds: Optional[torch.Tensor] = None,\n",
        "        labels: Optional[torch.Tensor] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        return_dict: Optional[bool] = None, # to play nice with HF adjacent packages\n",
        "    ) -> ESMplusplusOutput:\n",
        "        \"\"\"Forward pass for token classification.\n",
        "\n",
        "        Args:\n",
        "            input_ids: Input token IDs\n",
        "            attention_mask: Attention mask\n",
        "            inputs_embeds: Optional precomputed embeddings\n",
        "            labels: Optional labels for token classification\n",
        "            output_hidden_states: Whether to return all hidden states\n",
        "            output_attentions: Whether to return attention weights\n",
        "\n",
        "        Returns:\n",
        "            ESMplusplusOutput containing loss, logits, and hidden states\n",
        "        \"\"\"\n",
        "        output = super().forward(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            labels=None,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states\n",
        "        )\n",
        "        x = output.last_hidden_state\n",
        "        logits = self.classifier(x)\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss = self.loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "\n",
        "        return ESMplusplusOutput(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            last_hidden_state=x,\n",
        "            hidden_states=output.hidden_states,\n",
        "        )\n"
      ],
      "metadata": {
        "id": "lMX9_L-aGdPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading from EvolutionaryScale ESMplusplus models and sequence tokenizer"
      ],
      "metadata": {
        "id": "vIx6IE5NGqk_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Loading from EvolutionaryScale\n",
        "@staticmethod\n",
        "@cache\n",
        "def data_root(model: str):\n",
        "    if \"INFRA_PROVIDER\" in os.environ:\n",
        "        return Path(\"\")\n",
        "    # Try to download from hugginface if it doesn't exist\n",
        "    if model.startswith(\"esmc-300\"):\n",
        "        path = Path(snapshot_download(repo_id=\"EvolutionaryScale/esmc-300m-2024-12\"))\n",
        "    elif model.startswith(\"esmc-600\"):\n",
        "        path = Path(snapshot_download(repo_id=\"EvolutionaryScale/esmc-600m-2024-12\"))\n",
        "    else:\n",
        "        raise ValueError(f\"{model=} is an invalid model name.\")\n",
        "    return path\n",
        "\n",
        "\n",
        "def ESMplusplus_300M(device: torch.device | str = \"cpu\", num_labels: int = 3):\n",
        "    with torch.device(device):\n",
        "        config = ESMplusplusConfig(\n",
        "            hidden_size=960,\n",
        "            num_attention_heads=15,\n",
        "            num_hidden_layers=30,\n",
        "            num_labels=num_labels,\n",
        "        )\n",
        "        model = ESMplusplusForMaskedLM(config)\n",
        "    state_dict = torch.load(\n",
        "        data_root(\"esmc-300\") / \"data/weights/esmc_300m_2024_12_v0.pth\",\n",
        "        map_location=device,\n",
        "    )\n",
        "    model.load_state_dict(state_dict)\n",
        "    return model\n",
        "\n",
        "\n",
        "def ESMplusplus_600M(device: torch.device | str = \"cpu\", num_labels: int = 3):\n",
        "    with torch.device(device):\n",
        "        config = ESMplusplusConfig(\n",
        "            hidden_size=1152,\n",
        "            num_attention_heads=18,\n",
        "            num_hidden_layers=36,\n",
        "            num_labels=num_labels,\n",
        "        )\n",
        "        model = ESMplusplusForMaskedLM(config)\n",
        "        # # Use ESMplusplusForSequenceClassification to create the model with classification head\n",
        "        # model = ESMplusplusForSequenceClassification(config)\n",
        "    state_dict = torch.load(\n",
        "        data_root(\"esmc-600\") / \"data/weights/esmc_600m_2024_12_v0.pth\",\n",
        "        map_location=device,\n",
        "    )\n",
        "    model.load_state_dict(state_dict)\n",
        "    return model\n",
        "\n",
        "\n",
        "### Tokenization\n",
        "SEQUENCE_VOCAB = [\n",
        "    \"<cls>\", \"<pad>\", \"<eos>\", \"<unk>\",\n",
        "    \"L\", \"A\", \"G\", \"V\", \"S\", \"E\", \"R\", \"T\", \"I\", \"D\", \"P\", \"K\",\n",
        "    \"Q\", \"N\", \"F\", \"Y\", \"M\", \"H\", \"W\", \"C\", \"X\", \"B\", \"U\", \"Z\",\n",
        "    \"O\", \".\", \"-\", \"|\",\n",
        "    \"<mask>\",\n",
        "]\n",
        "\n",
        "class EsmSequenceTokenizer(PreTrainedTokenizerFast):\n",
        "    model_input_names = [\"input_ids\", \"attention_mask\"]\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        unk_token=\"<unk>\",\n",
        "        cls_token=\"<cls>\",\n",
        "        pad_token=\"<pad>\",\n",
        "        mask_token=\"<mask>\",\n",
        "        eos_token=\"<eos>\",\n",
        "        chain_break_token=\"|\",\n",
        "        **kwargs,\n",
        "    ):\n",
        "        all_tokens = SEQUENCE_VOCAB\n",
        "        token_to_id = {tok: ind for ind, tok in enumerate(all_tokens)}\n",
        "\n",
        "        # a character-level tokenizer is the same as BPE with no token merges\n",
        "        bpe = BPE(token_to_id, merges=[], unk_token=unk_token)\n",
        "        tokenizer = Tokenizer(bpe)\n",
        "        special_tokens = [\n",
        "            cls_token,\n",
        "            pad_token,\n",
        "            mask_token,\n",
        "            eos_token,\n",
        "            chain_break_token,\n",
        "        ]\n",
        "        self.cb_token = chain_break_token\n",
        "        additional_special_tokens = [chain_break_token]\n",
        "\n",
        "        tokenizer.add_special_tokens(special_tokens)\n",
        "\n",
        "        # This is where we configure the automatic addition of special tokens when we call\n",
        "        # tokenizer(text, add_special_tokens=True). Note that you can also configure how two\n",
        "        # sequences are merged if you want.\n",
        "        tokenizer.post_processor = TemplateProcessing(  # type: ignore\n",
        "            single=\"<cls> $A <eos>\",\n",
        "            special_tokens=[\n",
        "                (\"<cls>\", tokenizer.token_to_id(\"<cls>\")),\n",
        "                (\"<eos>\", tokenizer.token_to_id(\"<eos>\")),\n",
        "            ],\n",
        "        )\n",
        "        super().__init__(\n",
        "            tokenizer_object=tokenizer,\n",
        "            unk_token=unk_token,\n",
        "            cls_token=cls_token,\n",
        "            pad_token=pad_token,\n",
        "            mask_token=mask_token,\n",
        "            eos_token=eos_token,\n",
        "            additional_special_tokens=additional_special_tokens,\n",
        "            **kwargs,\n",
        "        )\n",
        "\n",
        "    # These are a footgun, we never use the `bos` token anywhere so we're just overriding it here.\n",
        "    @property\n",
        "    def bos_token(self):\n",
        "        return self.cls_token\n",
        "\n",
        "    @property\n",
        "    def bos_token_id(self):\n",
        "        return self.cls_token_id\n",
        "\n",
        "    @property\n",
        "    def chain_break_token(self):\n",
        "        return self.cb_token\n",
        "\n",
        "    @property\n",
        "    def chain_break_token_id(self):\n",
        "        return self.convert_tokens_to_ids(self.chain_break_token)\n",
        "\n",
        "    @property\n",
        "    def all_token_ids(self):\n",
        "        return list(range(self.vocab_size))\n",
        "\n",
        "    @property\n",
        "    def special_token_ids(self):\n",
        "        return self.all_special_ids\n"
      ],
      "metadata": {
        "id": "gLpKZEQRGqvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuration class for ESM++ model"
      ],
      "metadata": {
        "id": "kDq6HhYjHh-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ESMplusplusConfig(PretrainedConfig):\n",
        "    \"\"\"Configuration class for ESM++ model.\n",
        "\n",
        "    Args:\n",
        "        vocab_size: Size of the vocabulary\n",
        "        hidden_size: Dimension of hidden layers\n",
        "        num_attention_heads: Number of attention heads\n",
        "        num_hidden_layers: Number of transformer layers\n",
        "        num_labels: Number of output labels for classification\n",
        "        problem_type: Type of problem - regression, single/multi label classification\n",
        "    \"\"\"\n",
        "    model_type = \"ESMplusplus\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int = 64,\n",
        "        hidden_size: int = 960,\n",
        "        num_attention_heads: int = 15,\n",
        "        num_hidden_layers: int = 30,\n",
        "        num_labels: int = 2,\n",
        "        problem_type: str | None = None,\n",
        "        dropout: float = 0.0,\n",
        "        initializer_range: float = 0.02,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_attention_heads = num_attention_heads\n",
        "        self.num_hidden_layers = num_hidden_layers\n",
        "        self.num_labels = num_labels\n",
        "        self.problem_type = problem_type\n",
        "        self.dropout = dropout\n",
        "        self.initializer_range = initializer_range\n",
        "\n",
        "\n",
        "### Rotary Embeddings\n",
        "def rotate_half(x: torch.Tensor, interleaved: bool = False) -> torch.Tensor:\n",
        "    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
        "    if not interleaved:\n",
        "        x1, x2 = x.chunk(2, dim=-1)\n",
        "        return torch.cat((-x2, x1), dim=-1)\n",
        "    else:\n",
        "        x1, x2 = x[..., ::2], x[..., 1::2]\n",
        "        return rearrange(\n",
        "            torch.stack((-x2, x1), dim=-1), \"... d two -> ... (d two)\", two=2\n",
        "        )\n",
        "\n",
        "\n",
        "def apply_rotary_emb_torch(\n",
        "    x: torch.Tensor,\n",
        "    cos: torch.Tensor,\n",
        "    sin: torch.Tensor,\n",
        "    interleaved: bool = False,\n",
        "    _inplace: bool = False,\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"Apply rotary embeddings to input based on cos and sin.\"\"\"\n",
        "    ro_dim = cos.shape[-1] * 2\n",
        "    assert ro_dim <= x.shape[-1]\n",
        "    seqlen = x.size(1)\n",
        "    cos = cos[:seqlen]\n",
        "    sin = sin[:seqlen]\n",
        "    cos = repeat(cos, \"s d -> s 1 (2 d)\")\n",
        "    sin = repeat(sin, \"s d -> s 1 (2 d)\")\n",
        "    return torch.cat(\n",
        "        [\n",
        "            x[..., :ro_dim] * cos + rotate_half(x[..., :ro_dim], interleaved) * sin,\n",
        "            x[..., ro_dim:],\n",
        "        ],\n",
        "        dim=-1,\n",
        "    )\n",
        "\n",
        "\n",
        "class RotaryEmbedding(torch.nn.Module):\n",
        "    \"\"\"Rotary position embeddings.\n",
        "\n",
        "    Based on the paper \"RoFormer: Enhanced Transformer with Rotary Position Embedding\"\n",
        "\n",
        "    Args:\n",
        "        dim: Dimension of the embedding\n",
        "        base: Base for computing angular frequencies\n",
        "        interleaved: Whether to use interleaved rotations\n",
        "        scale_base: Base for scaling\n",
        "        scaling_factor: Factor for scaling positions\n",
        "        pos_idx_in_fp32: Whether to compute position indices in fp32\n",
        "        device: Computation device\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int,\n",
        "        base: float = 10000.0,\n",
        "        interleaved: bool = False,\n",
        "        scale_base: Optional[float] = None,\n",
        "        scaling_factor: float = 1.0,\n",
        "        pos_idx_in_fp32: bool = True,\n",
        "        device: Optional[torch.device] = None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.base = float(base)\n",
        "        self.pos_idx_in_fp32 = pos_idx_in_fp32\n",
        "        self.interleaved = interleaved\n",
        "        self.scale_base = scale_base\n",
        "        self.scaling_factor = scaling_factor\n",
        "        self.device = device\n",
        "\n",
        "        self._seq_len_cached = 0\n",
        "        self._cos_cached = None\n",
        "        self._sin_cached = None\n",
        "        self._cos_k_cached = None\n",
        "        self._sin_k_cached = None\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        \"\"\"Reset the parameters of the embedding.\"\"\"\n",
        "        inv_freq = self._compute_inv_freq(self.device)\n",
        "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
        "        arange = torch.arange(0, self.dim, 2, device=self.device, dtype=torch.float32)\n",
        "        scale = (\n",
        "            (arange + 0.4 * self.dim) / (1.4 * self.dim)\n",
        "            if self.scale_base is not None\n",
        "            else None\n",
        "        )\n",
        "        self.register_buffer(\"scale\", scale)\n",
        "\n",
        "    def _compute_inv_freq(self, device: Optional[torch.device] = None) -> torch.Tensor:\n",
        "        \"\"\"Compute inverse frequency bands.\"\"\"\n",
        "        return 1 / (\n",
        "            self.base\n",
        "            ** (\n",
        "                torch.arange(0, self.dim, 2, device=device, dtype=torch.float32)\n",
        "                / self.dim\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def _update_cos_sin_cache(self, seqlen: int, device: Optional[torch.device] = None, dtype: Optional[torch.dtype] = None):\n",
        "        \"\"\"Update the cached cosine and sine values.\"\"\"\n",
        "        if (\n",
        "            seqlen > self._seq_len_cached\n",
        "            or self._cos_cached is None\n",
        "            or self._cos_cached.device != device\n",
        "            or self._cos_cached.dtype != dtype\n",
        "            or (self.training and self._cos_cached.is_inference())\n",
        "        ):\n",
        "            self._seq_len_cached = seqlen\n",
        "            if self.pos_idx_in_fp32:\n",
        "                t = torch.arange(seqlen, device=device, dtype=torch.float32)\n",
        "                t /= self.scaling_factor\n",
        "                if self.inv_freq.dtype != torch.float32:\n",
        "                    inv_freq = self.inv_freq.to(torch.float32)\n",
        "                else:\n",
        "                    inv_freq = self.inv_freq\n",
        "            else:\n",
        "                t = torch.arange(seqlen, device=device, dtype=self.inv_freq.dtype)\n",
        "                t /= self.scaling_factor\n",
        "                inv_freq = self.inv_freq\n",
        "            freqs = torch.outer(t, inv_freq)\n",
        "\n",
        "            if self.scale is None:\n",
        "                self._cos_cached = torch.cos(freqs).to(dtype)\n",
        "                self._sin_cached = torch.sin(freqs).to(dtype)\n",
        "            else:\n",
        "                power = (\n",
        "                    torch.arange(\n",
        "                        seqlen, dtype=self.scale.dtype, device=self.scale.device\n",
        "                    )\n",
        "                    - seqlen // 2\n",
        "                ) / self.scale_base\n",
        "                scale = self.scale.to(device=power.device) ** power.unsqueeze(-1)\n",
        "                self._cos_cached = (torch.cos(freqs) * scale).to(dtype)\n",
        "                self._sin_cached = (torch.sin(freqs) * scale).to(dtype)\n",
        "                self._cos_k_cached = (torch.cos(freqs) / scale).to(dtype)\n",
        "                self._sin_k_cached = (torch.sin(freqs) / scale).to(dtype)\n",
        "\n",
        "    def forward(self, q: torch.Tensor, k: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Apply rotary embeddings to queries and keys.\n",
        "\n",
        "        Args:\n",
        "            q: Query tensor of shape (batch, seqlen, nheads, headdim)\n",
        "            k: Key tensor of shape (batch, seqlen, nheads, headdim)\n",
        "\n",
        "        Returns:\n",
        "            Tuple of rotated query and key tensors\n",
        "        \"\"\"\n",
        "        self._update_cos_sin_cache(q.shape[1], device=q.device, dtype=q.dtype)\n",
        "        assert self._cos_cached is not None\n",
        "        assert self._sin_cached is not None\n",
        "        if self.scale is None:\n",
        "            return (\n",
        "                apply_rotary_emb_torch(\n",
        "                    q,\n",
        "                    self._cos_cached,\n",
        "                    self._sin_cached,\n",
        "                    self.interleaved,\n",
        "                    True,  # inplace=True\n",
        "                ),\n",
        "                apply_rotary_emb_torch(\n",
        "                    k,\n",
        "                    self._cos_cached,\n",
        "                    self._sin_cached,\n",
        "                    self.interleaved,\n",
        "                    True,  # inplace=True\n",
        "                ),\n",
        "            )  # type: ignore\n",
        "        else:\n",
        "            assert False\n",
        "\n",
        "\n",
        "### Feedforward Network Components\n",
        "def swiglu_correction_fn(expansion_ratio: float, d_model: int) -> int:\n",
        "    \"\"\"Compute corrected dimension for SwiGLU.\"\"\"\n",
        "    return int(((expansion_ratio * d_model) + 255) // 256 * 256)\n",
        "\n",
        "\n",
        "class SwiGLU(nn.Module):\n",
        "    \"\"\"SwiGLU activation function.\"\"\"\n",
        "    def __init__(self):\n",
        "        super(SwiGLU, self).__init__()\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x1, x2 = x.chunk(2, dim=-1)\n",
        "        return F.silu(x1) * x2\n",
        "\n",
        "\n",
        "def swiglu_ln_ffn(d_model: int, expansion_ratio: float) -> nn.Sequential:\n",
        "    \"\"\"Create SwiGLU feedforward network with layer normalization.\"\"\"\n",
        "    return nn.Sequential(\n",
        "        nn.LayerNorm(d_model),\n",
        "        nn.Linear(\n",
        "            d_model, swiglu_correction_fn(expansion_ratio, d_model) * 2, bias=False\n",
        "        ),\n",
        "        SwiGLU(),\n",
        "        nn.Linear(swiglu_correction_fn(expansion_ratio, d_model), d_model, bias=False),\n",
        "    )\n",
        "\n",
        "\n",
        "### Multi-head attention with rotary embeddings\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"Multi-head attention with rotary embeddings.\n",
        "\n",
        "    Args:\n",
        "        d_model: Model dimension\n",
        "        n_heads: Number of attention heads\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model: int, n_heads: int):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.d_head = self.d_model // self.n_heads\n",
        "        self.layernorm_qkv = nn.Sequential(\n",
        "            nn.LayerNorm(d_model), nn.Linear(d_model, d_model * 3, bias=False)\n",
        "        )\n",
        "        self.out_proj = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.q_ln = nn.LayerNorm(d_model, bias=False)\n",
        "        self.k_ln = nn.LayerNorm(d_model, bias=False)\n",
        "        self.reshaper = partial(rearrange, pattern=\"b s (h d) -> b h s d\", h=n_heads)\n",
        "        self.rotary = RotaryEmbedding(d_model // n_heads)\n",
        "\n",
        "    def _apply_rotary(self, q: torch.Tensor, k: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Apply rotary embeddings to query and key.\"\"\"\n",
        "        q = q.unflatten(-1, (self.n_heads, self.d_head))\n",
        "        k = k.unflatten(-1, (self.n_heads, self.d_head))\n",
        "        q, k = self.rotary(q, k)\n",
        "        q = q.flatten(-2, -1)\n",
        "        k = k.flatten(-2, -1)\n",
        "        return q, k\n",
        "\n",
        "    def forward(self, x: torch.Tensor, attention_mask: Optional[torch.Tensor] = None, output_attentions: bool = False) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input tensor\n",
        "            attention_mask: Optional attention mask\n",
        "            output_attentions: Whether to return attention weights\n",
        "\n",
        "        Returns:\n",
        "            Output tensor after self attention, and optionally attention weights\n",
        "        \"\"\"\n",
        "        attn_weights = None\n",
        "        qkv_BLD3 = self.layernorm_qkv(x)\n",
        "        query_BLD, key_BLD, value_BLD = torch.chunk(qkv_BLD3, 3, dim=-1)\n",
        "        query_BLD, key_BLD = (\n",
        "            self.q_ln(query_BLD).to(query_BLD.dtype),\n",
        "            self.k_ln(key_BLD).to(query_BLD.dtype),\n",
        "        )\n",
        "        query_BLD, key_BLD = self._apply_rotary(query_BLD, key_BLD)\n",
        "        query_BHLD, key_BHLD, value_BHLD = map(self.reshaper, (query_BLD, key_BLD, value_BLD))\n",
        "\n",
        "        if output_attentions: # Manual attention computation\n",
        "            L, S = query_BLD.size(-2), key_BLD.size(-2)\n",
        "            scale = 1 / math.sqrt(query_BLD.size(-1))\n",
        "            attn_bias = torch.zeros(L, S, dtype=query_BLD.dtype, device=query_BLD.device)\n",
        "            if attention_mask is not None:\n",
        "                if attention_mask.dtype == torch.bool:\n",
        "                    attention_mask.masked_fill_(attention_mask.logical_not(), float('-inf'))\n",
        "                else:\n",
        "                    attn_bias += attention_mask\n",
        "\n",
        "            attn_weights = torch.matmul(query_BHLD, key_BHLD.transpose(-2, -1)) * scale\n",
        "            attn_weights += attn_bias\n",
        "            attn_weights = F.softmax(attn_weights, dim=-1)\n",
        "            context_BHLD = torch.matmul(attn_weights, value_BHLD)\n",
        "        else:\n",
        "            context_BHLD = F.scaled_dot_product_attention(\n",
        "                query_BHLD, key_BHLD, value_BHLD, attention_mask\n",
        "            )\n",
        "\n",
        "        context_BLD = rearrange(context_BHLD, \"b h s d -> b s (h d)\")\n",
        "        output = self.out_proj(context_BLD)\n",
        "        return output, attn_weights\n",
        "\n",
        "\n",
        "### Regression Head\n",
        "def RegressionHead(d_model: int, output_dim: int, hidden_dim: Optional[int] = None) -> nn.Module:\n",
        "    \"\"\"Create a regression head with optional hidden dimension.\n",
        "\n",
        "    Args:\n",
        "        d_model: Input dimension\n",
        "        output_dim: Output dimension\n",
        "        hidden_dim: Optional hidden dimension (defaults to d_model)\n",
        "    \"\"\"\n",
        "    hidden_dim = hidden_dim if hidden_dim is not None else d_model\n",
        "    return nn.Sequential(\n",
        "        nn.Linear(d_model, hidden_dim),\n",
        "        nn.GELU(),\n",
        "        nn.LayerNorm(hidden_dim),\n",
        "        nn.Linear(hidden_dim, output_dim),\n",
        "    )\n",
        "\n"
      ],
      "metadata": {
        "id": "f8wBCbMHHeph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rotary position embeddings"
      ],
      "metadata": {
        "id": "JbMS1tkwHsjz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RotaryEmbedding(torch.nn.Module):\n",
        "    \"\"\"Rotary position embeddings.\n",
        "\n",
        "    Based on the paper \"RoFormer: Enhanced Transformer with Rotary Position Embedding\"\n",
        "\n",
        "    Args:\n",
        "        dim: Dimension of the embedding\n",
        "        base: Base for computing angular frequencies\n",
        "        interleaved: Whether to use interleaved rotations\n",
        "        scale_base: Base for scaling\n",
        "        scaling_factor: Factor for scaling positions\n",
        "        pos_idx_in_fp32: Whether to compute position indices in fp32\n",
        "        device: Computation device\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int,\n",
        "        base: float = 10000.0,\n",
        "        interleaved: bool = False,\n",
        "        scale_base: Optional[float] = None,\n",
        "        scaling_factor: float = 1.0,\n",
        "        pos_idx_in_fp32: bool = True,\n",
        "        device: Optional[torch.device] = None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.base = float(base)\n",
        "        self.pos_idx_in_fp32 = pos_idx_in_fp32\n",
        "        self.interleaved = interleaved\n",
        "        self.scale_base = scale_base\n",
        "        self.scaling_factor = scaling_factor\n",
        "        self.device = device\n",
        "\n",
        "        self._seq_len_cached = 0\n",
        "        self._cos_cached = None\n",
        "        self._sin_cached = None\n",
        "        self._cos_k_cached = None\n",
        "        self._sin_k_cached = None\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        \"\"\"Reset the parameters of the embedding.\"\"\"\n",
        "        inv_freq = self._compute_inv_freq(self.device)\n",
        "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
        "        arange = torch.arange(0, self.dim, 2, device=self.device, dtype=torch.float32)\n",
        "        scale = (\n",
        "            (arange + 0.4 * self.dim) / (1.4 * self.dim)\n",
        "            if self.scale_base is not None\n",
        "            else None\n",
        "        )\n",
        "        self.register_buffer(\"scale\", scale)\n",
        "\n",
        "    def _compute_inv_freq(self, device: Optional[torch.device] = None) -> torch.Tensor:\n",
        "        \"\"\"Compute inverse frequency bands.\"\"\"\n",
        "        return 1 / (\n",
        "            self.base\n",
        "            ** (\n",
        "                torch.arange(0, self.dim, 2, device=device, dtype=torch.float32)\n",
        "                / self.dim\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def _update_cos_sin_cache(self, seqlen: int, device: Optional[torch.device] = None, dtype: Optional[torch.dtype] = None):\n",
        "        \"\"\"Update the cached cosine and sine values.\"\"\"\n",
        "        if (\n",
        "            seqlen > self._seq_len_cached\n",
        "            or self._cos_cached is None\n",
        "            or self._cos_cached.device != device\n",
        "            or self._cos_cached.dtype != dtype\n",
        "            or (self.training and self._cos_cached.is_inference())\n",
        "        ):\n",
        "            self._seq_len_cached = seqlen\n",
        "            if self.pos_idx_in_fp32:\n",
        "                t = torch.arange(seqlen, device=device, dtype=torch.float32)\n",
        "                t /= self.scaling_factor\n",
        "                if self.inv_freq.dtype != torch.float32:\n",
        "                    inv_freq = self.inv_freq.to(torch.float32)\n",
        "                else:\n",
        "                    inv_freq = self.inv_freq\n",
        "            else:\n",
        "                t = torch.arange(seqlen, device=device, dtype=self.inv_freq.dtype)\n",
        "                t /= self.scaling_factor\n",
        "                inv_freq = self.inv_freq\n",
        "            freqs = torch.outer(t, inv_freq)\n",
        "\n",
        "            if self.scale is None:\n",
        "                self._cos_cached = torch.cos(freqs).to(dtype)\n",
        "                self._sin_cached = torch.sin(freqs).to(dtype)\n",
        "            else:\n",
        "                power = (\n",
        "                    torch.arange(\n",
        "                        seqlen, dtype=self.scale.dtype, device=self.scale.device\n",
        "                    )\n",
        "                    - seqlen // 2\n",
        "                ) / self.scale_base\n",
        "                scale = self.scale.to(device=power.device) ** power.unsqueeze(-1)\n",
        "                self._cos_cached = (torch.cos(freqs) * scale).to(dtype)\n",
        "                self._sin_cached = (torch.sin(freqs) * scale).to(dtype)\n",
        "                self._cos_k_cached = (torch.cos(freqs) / scale).to(dtype)\n",
        "                self._sin_k_cached = (torch.sin(freqs) / scale).to(dtype)\n",
        "\n",
        "    def forward(self, q: torch.Tensor, k: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Apply rotary embeddings to queries and keys.\n",
        "\n",
        "        Args:\n",
        "            q: Query tensor of shape (batch, seqlen, nheads, headdim)\n",
        "            k: Key tensor of shape (batch, seqlen, nheads, headdim)\n",
        "\n",
        "        Returns:\n",
        "            Tuple of rotated query and key tensors\n",
        "        \"\"\"\n",
        "        self._update_cos_sin_cache(q.shape[1], device=q.device, dtype=q.dtype)\n",
        "        assert self._cos_cached is not None\n",
        "        assert self._sin_cached is not None\n",
        "        if self.scale is None:\n",
        "            return (\n",
        "                apply_rotary_emb_torch(\n",
        "                    q,\n",
        "                    self._cos_cached,\n",
        "                    self._sin_cached,\n",
        "                    self.interleaved,\n",
        "                    True,  # inplace=True\n",
        "                ),\n",
        "                apply_rotary_emb_torch(\n",
        "                    k,\n",
        "                    self._cos_cached,\n",
        "                    self._sin_cached,\n",
        "                    self.interleaved,\n",
        "                    True,  # inplace=True\n",
        "                ),\n",
        "            )  # type: ignore\n",
        "        else:\n",
        "            assert False\n",
        "\n",
        "\n",
        "### Feedforward Network Components\n",
        "def swiglu_correction_fn(expansion_ratio: float, d_model: int) -> int:\n",
        "    \"\"\"Compute corrected dimension for SwiGLU.\"\"\"\n",
        "    return int(((expansion_ratio * d_model) + 255) // 256 * 256)\n",
        "\n",
        "\n",
        "class SwiGLU(nn.Module):\n",
        "    \"\"\"SwiGLU activation function.\"\"\"\n",
        "    def __init__(self):\n",
        "        super(SwiGLU, self).__init__()\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x1, x2 = x.chunk(2, dim=-1)\n",
        "        return F.silu(x1) * x2\n",
        "\n",
        "\n",
        "def swiglu_ln_ffn(d_model: int, expansion_ratio: float) -> nn.Sequential:\n",
        "    \"\"\"Create SwiGLU feedforward network with layer normalization.\"\"\"\n",
        "    return nn.Sequential(\n",
        "        nn.LayerNorm(d_model),\n",
        "        nn.Linear(\n",
        "            d_model, swiglu_correction_fn(expansion_ratio, d_model) * 2, bias=False\n",
        "        ),\n",
        "        SwiGLU(),\n",
        "        nn.Linear(swiglu_correction_fn(expansion_ratio, d_model), d_model, bias=False),\n",
        "    )\n",
        "\n"
      ],
      "metadata": {
        "id": "ngxwsBabHstk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-head attention with rotary embeddings"
      ],
      "metadata": {
        "id": "WPwmMLPVIF8A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Attention\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"Multi-head attention with rotary embeddings.\n",
        "\n",
        "    Args:\n",
        "        d_model: Model dimension\n",
        "        n_heads: Number of attention heads\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model: int, n_heads: int):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.d_head = self.d_model // self.n_heads\n",
        "        self.layernorm_qkv = nn.Sequential(\n",
        "            nn.LayerNorm(d_model), nn.Linear(d_model, d_model * 3, bias=False)\n",
        "        )\n",
        "        self.out_proj = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.q_ln = nn.LayerNorm(d_model, bias=False)\n",
        "        self.k_ln = nn.LayerNorm(d_model, bias=False)\n",
        "        self.reshaper = partial(rearrange, pattern=\"b s (h d) -> b h s d\", h=n_heads)\n",
        "        self.rotary = RotaryEmbedding(d_model // n_heads)\n",
        "\n",
        "    def _apply_rotary(self, q: torch.Tensor, k: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Apply rotary embeddings to query and key.\"\"\"\n",
        "        q = q.unflatten(-1, (self.n_heads, self.d_head))\n",
        "        k = k.unflatten(-1, (self.n_heads, self.d_head))\n",
        "        q, k = self.rotary(q, k)\n",
        "        q = q.flatten(-2, -1)\n",
        "        k = k.flatten(-2, -1)\n",
        "        return q, k\n",
        "\n",
        "    def forward(self, x: torch.Tensor, attention_mask: Optional[torch.Tensor] = None, output_attentions: bool = False) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input tensor\n",
        "            attention_mask: Optional attention mask\n",
        "            output_attentions: Whether to return attention weights\n",
        "\n",
        "        Returns:\n",
        "            Output tensor after self attention, and optionally attention weights\n",
        "        \"\"\"\n",
        "        attn_weights = None\n",
        "        qkv_BLD3 = self.layernorm_qkv(x)\n",
        "        query_BLD, key_BLD, value_BLD = torch.chunk(qkv_BLD3, 3, dim=-1)\n",
        "        query_BLD, key_BLD = (\n",
        "            self.q_ln(query_BLD).to(query_BLD.dtype),\n",
        "            self.k_ln(key_BLD).to(query_BLD.dtype),\n",
        "        )\n",
        "        query_BLD, key_BLD = self._apply_rotary(query_BLD, key_BLD)\n",
        "        query_BHLD, key_BHLD, value_BHLD = map(self.reshaper, (query_BLD, key_BLD, value_BLD))\n",
        "\n",
        "        if output_attentions: # Manual attention computation\n",
        "            L, S = query_BLD.size(-2), key_BLD.size(-2)\n",
        "            scale = 1 / math.sqrt(query_BLD.size(-1))\n",
        "            attn_bias = torch.zeros(L, S, dtype=query_BLD.dtype, device=query_BLD.device)\n",
        "            if attention_mask is not None:\n",
        "                if attention_mask.dtype == torch.bool:\n",
        "                    attention_mask.masked_fill_(attention_mask.logical_not(), float('-inf'))\n",
        "                else:\n",
        "                    attn_bias += attention_mask\n",
        "\n",
        "            attn_weights = torch.matmul(query_BHLD, key_BHLD.transpose(-2, -1)) * scale\n",
        "            attn_weights += attn_bias\n",
        "            attn_weights = F.softmax(attn_weights, dim=-1)\n",
        "            context_BHLD = torch.matmul(attn_weights, value_BHLD)\n",
        "        else:\n",
        "            context_BHLD = F.scaled_dot_product_attention(\n",
        "                query_BHLD, key_BHLD, value_BHLD, attention_mask\n",
        "            )\n",
        "\n",
        "        context_BLD = rearrange(context_BHLD, \"b h s d -> b s (h d)\")\n",
        "        output = self.out_proj(context_BLD)\n",
        "        return output, attn_weights\n",
        "\n",
        "\n",
        "### Regression Head\n",
        "def RegressionHead(d_model: int, output_dim: int, hidden_dim: Optional[int] = None) -> nn.Module:\n",
        "    \"\"\"Create a regression head with optional hidden dimension.\n",
        "\n",
        "    Args:\n",
        "        d_model: Input dimension\n",
        "        output_dim: Output dimension\n",
        "        hidden_dim: Optional hidden dimension (defaults to d_model)\n",
        "    \"\"\"\n",
        "    hidden_dim = hidden_dim if hidden_dim is not None else d_model\n",
        "    return nn.Sequential(\n",
        "        nn.Linear(d_model, hidden_dim),\n",
        "        nn.GELU(),\n",
        "        nn.LayerNorm(hidden_dim),\n",
        "        nn.Linear(hidden_dim, output_dim),\n",
        "    )\n"
      ],
      "metadata": {
        "id": "ruVT9frvIM2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer block with attention and feedforward layers"
      ],
      "metadata": {
        "id": "-bzFNmorIZtM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Transformer Block\n",
        "class UnifiedTransformerBlock(nn.Module):\n",
        "    \"\"\"Transformer block with attention and feedforward layers.\n",
        "\n",
        "    Args:\n",
        "        d_model: Model dimension\n",
        "        n_heads: Number of attention heads\n",
        "        residue_scaling_factor: Factor for scaling residual connections\n",
        "        expansion_ratio: Expansion ratio for feedforward network\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        d_model: int,\n",
        "        n_heads: int,\n",
        "        residue_scaling_factor: float = 1,\n",
        "        expansion_ratio: float = 8 / 3,\n",
        "        dropout: float = 0.0,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.attn = MultiHeadAttention(d_model, n_heads)\n",
        "        self.ffn = swiglu_ln_ffn(d_model, expansion_ratio)\n",
        "        self.scaling_factor = residue_scaling_factor\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x: torch.Tensor,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        output_attentions: bool = False,\n",
        "    ) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input tensor\n",
        "            attention_mask: Optional attention mask\n",
        "            output_attentions: Whether to return attention weights\n",
        "\n",
        "        Returns:\n",
        "            Output tensor after transformer block, and optionally attention weights\n",
        "        \"\"\"\n",
        "        attn_output, attn_weights = self.attn(x, attention_mask, output_attentions)\n",
        "        x = x + self.dropout(attn_output) / self.scaling_factor\n",
        "        x = x + self.dropout(self.ffn(x)) / self.scaling_factor\n",
        "        return x, attn_weights\n"
      ],
      "metadata": {
        "id": "UfwlvOytIZz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model output types"
      ],
      "metadata": {
        "id": "Bm_uUCPZIivY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Model Outputs\n",
        "@dataclass\n",
        "class TransformerOutput(ModelOutput):\n",
        "    \"\"\"Output type for transformer encoder.\"\"\"\n",
        "    last_hidden_state: Optional[torch.Tensor] = None\n",
        "    hidden_states: Optional[Tuple[torch.Tensor]] = None\n",
        "    attentions: Optional[Tuple[torch.Tensor]] = None\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ESMplusplusOutput(ModelOutput):\n",
        "    \"\"\"Output type for ESM++ models.\"\"\"\n",
        "    loss: Optional[torch.Tensor] = None\n",
        "    logits: Optional[torch.Tensor] = None\n",
        "    last_hidden_state: Optional[torch.Tensor] = None\n",
        "    hidden_states: Optional[Tuple[torch.Tensor]] = None\n",
        "    attentions: Optional[Tuple[torch.Tensor]] = None\n",
        "\n",
        "\n",
        "### Transformer Stack\n",
        "class TransformerStack(nn.Module):\n",
        "    \"\"\"Stack of transformer blocks.\n",
        "\n",
        "    Args:\n",
        "        d_model: Model dimension\n",
        "        n_heads: Number of attention heads\n",
        "        n_layers: Number of transformer layers\n",
        "        dropout: Dropout rate\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        d_model: int,\n",
        "        n_heads: int,\n",
        "        n_layers: int,\n",
        "        dropout: float = 0.0,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.blocks = nn.ModuleList(\n",
        "            [\n",
        "                UnifiedTransformerBlock(\n",
        "                    d_model,\n",
        "                    n_heads,\n",
        "                    residue_scaling_factor=math.sqrt(n_layers / 36),\n",
        "                    dropout=dropout,\n",
        "                )\n",
        "                for i in range(n_layers)\n",
        "            ]\n",
        "        )\n",
        "        self.norm = nn.LayerNorm(d_model, bias=False)\n",
        "        self.gradient_checkpointing = False\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x: torch.Tensor,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        output_hidden_states: bool = False,\n",
        "        output_attentions: bool = False,\n",
        "    ) -> TransformerOutput:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input tensor\n",
        "            attention_mask: Optional attention mask\n",
        "            output_hidden_states: Whether to return all hidden states\n",
        "            output_attentions: Whether to return attention weights\n",
        "\n",
        "        Returns:\n",
        "            TransformerOutput containing last hidden state and optionally all hidden states and attention weights\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "        hidden_states = () if output_hidden_states else None\n",
        "        attentions = () if output_attentions else None\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            attention_mask = attention_mask[:, None, None, :].expand(batch_size, 1, seq_len, seq_len).bool()\n",
        "\n",
        "        for block in self.blocks:\n",
        "            if self.gradient_checkpointing and self.training:\n",
        "                x, attn_weights = self._gradient_checkpointing_func(\n",
        "                    block.__call__,\n",
        "                    x,\n",
        "                    attention_mask,\n",
        "                    output_attentions,\n",
        "                )\n",
        "            else:\n",
        "                x, attn_weights = block(x, attention_mask, output_attentions)\n",
        "\n",
        "            if attentions is not None:\n",
        "                attentions += (attn_weights,)\n",
        "\n",
        "            if output_hidden_states:\n",
        "                assert hidden_states is not None\n",
        "                hidden_states += (x,)\n",
        "\n",
        "        return TransformerOutput(\n",
        "            last_hidden_state=self.norm(x),\n",
        "            hidden_states=hidden_states,\n",
        "            attentions=attentions\n",
        "        )\n"
      ],
      "metadata": {
        "id": "zMN6W-z_Ii3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset for Protein Sequences Embedding"
      ],
      "metadata": {
        "id": "nO3x8gqXI197"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Dataset for Embedding\n",
        "class ProteinDataset(Dataset):\n",
        "    \"\"\"Simple dataset for protein sequences.\"\"\"\n",
        "    def __init__(self, sequences: list[str]):\n",
        "        self.sequences = sequences\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> str:\n",
        "        return self.sequences[idx]\n",
        "\n",
        "\n",
        "class PreTrainedESMplusplusModel(PreTrainedModel):\n",
        "    \"\"\"\n",
        "    init weights for ESM++ models\n",
        "    \"\"\"\n",
        "    config_class = ESMplusplusConfig\n",
        "    base_model_prefix = \"esm++\"\n",
        "    supports_gradient_checkpointing = True\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        \"\"\"Initialize the weights\"\"\"\n",
        "        if isinstance(module, nn.Linear):\n",
        "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "            if module.padding_idx is not None:\n",
        "                module.weight.data[module.padding_idx].zero_()\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "            module.weight.data.fill_(1.0)\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained_esm(cls, model_name: str):\n",
        "        \"\"\"Load a pretrained ESM++ model.\"\"\"\n",
        "        if '300' in model_name:\n",
        "            return ESMplusplus_300M()\n",
        "        elif '600' in model_name:\n",
        "            return ESMplusplus_600M()\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid model name: {model_name}\")\n",
        "\n",
        "    @property\n",
        "    def device(self) -> torch.device:\n",
        "        \"\"\"Get the device of the model.\"\"\"\n",
        "        return next(self.parameters()).device\n",
        "\n",
        "    def mean_pooling(self, x: torch.Tensor, attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
        "        \"\"\"Apply mean pooling to sequence outputs.\"\"\"\n",
        "        if attention_mask is None:\n",
        "            return x.mean(dim=1)\n",
        "        else:\n",
        "            attention_mask = attention_mask.unsqueeze(-1)\n",
        "            return (x * attention_mask).sum(dim=1) / attention_mask.sum(dim=1)\n",
        "\n",
        "    def max_pooling(self, x: torch.Tensor, attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
        "        \"\"\"Apply max pooling to sequence outputs.\"\"\"\n",
        "        if attention_mask is None:\n",
        "            return x.max(dim=1).values\n",
        "        else:\n",
        "            attention_mask = attention_mask.unsqueeze(-1)\n",
        "            return (x * attention_mask).max(dim=1).values\n",
        "\n",
        "    def cls_pooling(self, x: torch.Tensor, attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
        "        \"\"\"Apply cls pooling to sequence outputs.\"\"\"\n",
        "        return x[:, 0, :]\n",
        "\n",
        "    def _collate_fn(self, sequences: list[str]) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Collate function for batching sequences.\"\"\"\n",
        "        return self.tokenizer(sequences, return_tensors=\"pt\", padding='longest', pad_to_multiple_of=8)\n",
        "\n",
        "    def _read_sequences_from_db(self, db_path: str) -> set[str]:\n",
        "        \"\"\"Read sequences from SQLite database.\"\"\"\n",
        "        import sqlite3\n",
        "        sequences = []\n",
        "        with sqlite3.connect(db_path) as conn:\n",
        "            c = conn.cursor()\n",
        "            c.execute(\"SELECT sequence FROM embeddings\")\n",
        "            while True:\n",
        "                row = c.fetchone()\n",
        "                if row is None:\n",
        "                    break\n",
        "                sequences.append(row[0])\n",
        "        return set(sequences)\n",
        "\n",
        "    def embed_dataset(\n",
        "        self,\n",
        "        sequences: list[str],\n",
        "        batch_size: int = 2,\n",
        "        max_len: int = 512,\n",
        "        full_embeddings: bool = False,\n",
        "        full_precision: bool = False,\n",
        "        pooling_type: str = 'mean',\n",
        "        num_workers: int = 0,\n",
        "        sql: bool = False,\n",
        "        sql_db_path: str = 'embeddings.db',\n",
        "    ) -> Optional[dict[str, torch.Tensor]]:\n",
        "        \"\"\"Embed a dataset of protein sequences.\n",
        "\n",
        "        Args:\n",
        "            sequences: List of protein sequences\n",
        "            batch_size: Batch size for processing\n",
        "            max_len: Maximum sequence length\n",
        "            full_embeddings: Whether to return full residue-wise (True) embeddings or pooled (False)\n",
        "            full_precision: Whether to cast to full precision (float32) before storage - relevant for dict storage\n",
        "            pooling_type: Type of pooling ('mean' or 'cls')\n",
        "            num_workers: Number of workers for data loading, 0 for the main process\n",
        "            sql: Whether to store embeddings in SQLite database - will be stored in float32\n",
        "            sql_db_path: Path to SQLite database\n",
        "\n",
        "        Returns:\n",
        "            Dictionary mapping sequences to embeddings, or None if sql=True\n",
        "        \"\"\"\n",
        "        sequences = list(set([seq[:max_len] for seq in sequences]))\n",
        "        device = self.device\n",
        "\n",
        "        def get_embeddings(residue_embeddings: torch.Tensor, attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
        "            if full_embeddings:\n",
        "                return residue_embeddings\n",
        "            elif pooling_type == 'mean':\n",
        "                return self.mean_pooling(residue_embeddings, attention_mask)\n",
        "            elif pooling_type == 'max':\n",
        "                return self.max_pooling(residue_embeddings, attention_mask)\n",
        "            elif pooling_type == 'cls':\n",
        "                return self.cls_pooling(residue_embeddings, attention_mask)\n",
        "            else:\n",
        "                raise ValueError(f\"Invalid pooling type: {pooling_type}\")\n",
        "\n",
        "        sequences = list(set([seq[:max_len] for seq in sequences]))\n",
        "        if sql:\n",
        "            import sqlite3\n",
        "            conn = sqlite3.connect(sql_db_path)\n",
        "            c = conn.cursor()\n",
        "            c.execute('CREATE TABLE IF NOT EXISTS embeddings (sequence text PRIMARY KEY, embedding blob)')\n",
        "            already_embedded = self._read_sequences_from_db(sql_db_path)\n",
        "            to_embed = [seq for seq in sequences if seq not in already_embedded]\n",
        "            print(f\"Found {len(already_embedded)} already embedded sequences in {sql_db_path}\")\n",
        "            print(f\"Embedding {len(to_embed)} new sequences\")\n",
        "            if len(to_embed) > 0:\n",
        "                to_embed = sorted(to_embed, key=len, reverse=True)\n",
        "                dataset = ProteinDataset(to_embed)\n",
        "                dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=num_workers, collate_fn=self._collate_fn, shuffle=False)\n",
        "                with torch.no_grad():\n",
        "                    for i, batch in tqdm(enumerate(dataloader), total=len(dataloader), desc='Embedding batches'):\n",
        "                        seqs = to_embed[i * batch_size:(i + 1) * batch_size]\n",
        "                        input_ids, attention_mask = batch['input_ids'].to(device), batch['attention_mask'].to(device)\n",
        "                        x = self.embed(input_ids)\n",
        "                        residue_embeddings = self.transformer(x, attention_mask).last_hidden_state.detach().float() # required for sql\n",
        "                        embeddings = get_embeddings(residue_embeddings, attention_mask)\n",
        "\n",
        "                        for seq, emb, mask in zip(seqs, embeddings, attention_mask):\n",
        "                            if full_embeddings:\n",
        "                                emb = emb[mask.bool()]\n",
        "                            c.execute(\"INSERT OR REPLACE INTO embeddings VALUES (?, ?)\",\n",
        "                                    (seq, emb.cpu().numpy().tobytes()))\n",
        "\n",
        "                        if (i + 1) % 100 == 0:\n",
        "                            conn.commit()\n",
        "\n",
        "                conn.commit()\n",
        "            conn.close()\n",
        "            return None\n",
        "\n",
        "        embeddings_dict = {}\n",
        "        sequences = sorted(sequences, key=len, reverse=True)\n",
        "        dataset = ProteinDataset(sequences)\n",
        "        dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=num_workers, collate_fn=self._collate_fn, shuffle=False)\n",
        "        with torch.no_grad():\n",
        "            for i, batch in tqdm(enumerate(dataloader), total=len(dataloader), desc='Embedding batches'):\n",
        "                seqs = sequences[i * batch_size:(i + 1) * batch_size]\n",
        "                input_ids, attention_mask = batch['input_ids'].to(device), batch['attention_mask'].to(device)\n",
        "                x = self.embed(input_ids)\n",
        "                residue_embeddings = self.transformer(x, attention_mask).last_hidden_state.detach()\n",
        "                if full_precision:\n",
        "                    residue_embeddings = residue_embeddings.float()\n",
        "                embeddings = get_embeddings(residue_embeddings, attention_mask).cpu()\n",
        "                for seq, emb in zip(seqs, embeddings):\n",
        "                    embeddings_dict[seq] = emb\n",
        "\n",
        "        return embeddings_dict\n"
      ],
      "metadata": {
        "id": "dsLA9NQzI4eO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ESM++ Models with masked language modeling head"
      ],
      "metadata": {
        "id": "mi_gXlnVJOFh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### ESM++ Models\n",
        "class ESMplusplusModel(PreTrainedESMplusplusModel):\n",
        "    \"\"\"\n",
        "    ESM++ model. transformer model with no heads\n",
        "    \"\"\"\n",
        "    config_class = ESMplusplusConfig\n",
        "    def __init__(self, config: ESMplusplusConfig, **kwargs):\n",
        "        super().__init__(config, **kwargs)\n",
        "        self.config = config\n",
        "        self.vocab_size = config.vocab_size\n",
        "        self.embed = nn.Embedding(self.vocab_size, config.hidden_size)\n",
        "        self.transformer = TransformerStack(config.hidden_size, config.num_attention_heads, config.num_hidden_layers, config.dropout)\n",
        "        self.tokenizer = EsmSequenceTokenizer()\n",
        "        self.init_weights()\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.embed\n",
        "\n",
        "    def set_input_embeddings(self, value):\n",
        "        self.embed = value\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: Optional[torch.Tensor] = None,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        inputs_embeds: Optional[torch.Tensor] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        return_dict: Optional[bool] = None, # to play nice with HF adjacent packages\n",
        "    ) -> TransformerOutput:\n",
        "        \"\"\"Forward pass for masked language modeling.\n",
        "\n",
        "        Args:\n",
        "            input_ids: Input token IDs\n",
        "            attention_mask: Attention mask\n",
        "            inputs_embeds: Optional precomputed embeddings\n",
        "            output_hidden_states: Whether to return all hidden states\n",
        "            output_attentions: Whether to return attention weights\n",
        "\n",
        "        Returns:\n",
        "            TransformerOutput containing last hidden state and optionally all hidden states and attention weights\n",
        "        \"\"\"\n",
        "        if inputs_embeds is None:\n",
        "            x = self.embed(input_ids)\n",
        "        else:\n",
        "            x = inputs_embeds\n",
        "        return self.transformer(x, attention_mask, output_hidden_states, output_attentions)\n",
        "\n",
        "\n",
        "class ESMplusplusForMaskedLM(PreTrainedESMplusplusModel):\n",
        "    \"\"\"\n",
        "    ESM++ model for masked language modeling.\n",
        "    Implements the base ESM++ architecture with a masked language modeling head.\n",
        "    \"\"\"\n",
        "    config_class = ESMplusplusConfig\n",
        "    def __init__(self, config: ESMplusplusConfig, **kwargs):\n",
        "        super().__init__(config, **kwargs)\n",
        "        self.config = config\n",
        "        self.vocab_size = config.vocab_size\n",
        "        self.embed = nn.Embedding(self.vocab_size, config.hidden_size)\n",
        "        self.transformer = TransformerStack(config.hidden_size, config.num_attention_heads, config.num_hidden_layers, config.dropout)\n",
        "        self.sequence_head = RegressionHead(config.hidden_size, self.vocab_size)\n",
        "        self.ce_loss = nn.CrossEntropyLoss()\n",
        "        self.tokenizer = EsmSequenceTokenizer()\n",
        "        self.init_weights()\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.embed\n",
        "\n",
        "    def set_input_embeddings(self, value):\n",
        "        self.embed = value\n",
        "\n",
        "    def get_output_embeddings(self):\n",
        "        return self.sequence_head[-1]\n",
        "\n",
        "    def set_output_embeddings(self, new_embeddings):\n",
        "        self.sequence_head[-1] = new_embeddings\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: Optional[torch.Tensor] = None,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        inputs_embeds: Optional[torch.Tensor] = None,\n",
        "        labels: Optional[torch.Tensor] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        return_dict: Optional[bool] = None, # to play nice with HF adjacent packages\n",
        "    ) -> ESMplusplusOutput:\n",
        "        \"\"\"Forward pass for masked language modeling.\n",
        "\n",
        "        Args:\n",
        "            input_ids: Input token IDs\n",
        "            attention_mask: Attention mask\n",
        "            inputs_embeds: Optional precomputed embeddings\n",
        "            labels: Optional labels for masked tokens\n",
        "            output_hidden_states: Whether to return all hidden states\n",
        "            output_attentions: Whether to return attention weights\n",
        "\n",
        "        Returns:\n",
        "            ESMplusplusOutput containing loss, logits, hidden states and attention weights\n",
        "        \"\"\"\n",
        "        if inputs_embeds is None:\n",
        "            x = self.embed(input_ids)\n",
        "        else:\n",
        "            x = inputs_embeds\n",
        "        output = self.transformer(x, attention_mask, output_hidden_states, output_attentions)\n",
        "        x = output.last_hidden_state\n",
        "        logits = self.sequence_head(x)\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss = self.ce_loss(logits.view(-1, self.vocab_size), labels.view(-1))\n",
        "        return ESMplusplusOutput(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            last_hidden_state=x,\n",
        "            hidden_states=output.hidden_states,\n",
        "            attentions=output.attentions,\n",
        "        )\n"
      ],
      "metadata": {
        "id": "JeEh-T8BJOPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ESM++ model for sequence and token classification\n",
        "Extends the base ESM++ model with a classification head for either sequence or token classification tasks."
      ],
      "metadata": {
        "id": "WiPT7sAsJyMD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ESMplusplusForSequenceClassification(ESMplusplusForMaskedLM):\n",
        "    \"\"\"\n",
        "    ESM++ model for sequence classification.\n",
        "    Extends the base ESM++ model with a classification head.\n",
        "    \"\"\"\n",
        "    def __init__(self, config: ESMplusplusConfig, **kwargs):\n",
        "        super().__init__(config, **kwargs)\n",
        "        self.config = config\n",
        "        self.num_labels = config.num_labels\n",
        "        self.classifier = RegressionHead(config.hidden_size * 2, config.num_labels, config.hidden_size * 4)\n",
        "        # Large intermediate projections help with sequence classification tasks (*4)\n",
        "        self.mse = nn.MSELoss()\n",
        "        self.ce = nn.CrossEntropyLoss()\n",
        "        self.bce = nn.BCEWithLogitsLoss()\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: Optional[torch.Tensor] = None,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        inputs_embeds: Optional[torch.Tensor] = None,\n",
        "        labels: Optional[torch.Tensor] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        return_dict: Optional[bool] = None, # to play nice with HF adjacent packages\n",
        "    ) -> ESMplusplusOutput:\n",
        "        \"\"\"Forward pass for sequence classification.\n",
        "\n",
        "        Args:\n",
        "            input_ids: Input token IDs\n",
        "            attention_mask: Attention mask\n",
        "            inputs_embeds: Optional precomputed embeddings\n",
        "            labels: Optional labels for classification\n",
        "            output_hidden_states: Whether to return all hidden states\n",
        "            output_attentions: Whether to return attention weights\n",
        "\n",
        "        Returns:\n",
        "            ESMplusplusOutput containing loss, logits, and hidden states\n",
        "        \"\"\"\n",
        "        output = super().forward(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            labels=None,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states\n",
        "        )\n",
        "        x = output.last_hidden_state\n",
        "        cls_features = x[:, 0, :]\n",
        "        mean_features = self.mean_pooling(x, attention_mask)\n",
        "        # we include mean pooling features to help with early convergence, the cost of this is basically zero\n",
        "        features = torch.cat([cls_features, mean_features], dim=-1)\n",
        "        logits = self.classifier(features)\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            labels = labels.to(logits.device)\n",
        "            if self.config.problem_type is None:\n",
        "                if self.num_labels == 1:\n",
        "                    self.config.problem_type = \"regression\"\n",
        "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
        "                    self.config.problem_type = \"single_label_classification\"\n",
        "                else:\n",
        "                    self.config.problem_type = \"multi_label_classification\"\n",
        "\n",
        "            if self.config.problem_type == \"regression\":\n",
        "                if self.num_labels == 1:\n",
        "                    loss = self.mse(logits.flatten(), labels.flatten())\n",
        "                else:\n",
        "                    loss = self.mse(logits, labels)\n",
        "            elif self.config.problem_type == \"single_label_classification\":\n",
        "                loss = self.ce(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "            elif self.config.problem_type == \"multi_label_classification\":\n",
        "                loss = self.bce(logits, labels)\n",
        "\n",
        "        return ESMplusplusOutput(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            last_hidden_state=x,\n",
        "            hidden_states=output.hidden_states,\n",
        "        )\n",
        "\n",
        "\n",
        "class ESMplusplusForTokenClassification(ESMplusplusForMaskedLM):\n",
        "    \"\"\"\n",
        "    ESM++ model for token classification.\n",
        "    Extends the base ESM++ model with a token classification head.\n",
        "    \"\"\"\n",
        "    def __init__(self, config: ESMplusplusConfig):\n",
        "        super().__init__(config)\n",
        "        self.config = config\n",
        "        self.num_labels = config.num_labels\n",
        "        self.classifier = RegressionHead(config.hidden_size, config.num_labels, config.hidden_size * 4)\n",
        "        # Large intermediate projections help with sequence classification tasks (*4)\n",
        "        self.loss_fct = nn.CrossEntropyLoss()\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: Optional[torch.Tensor] = None,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        inputs_embeds: Optional[torch.Tensor] = None,\n",
        "        labels: Optional[torch.Tensor] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        return_dict: Optional[bool] = None, # to play nice with HF adjacent packages\n",
        "    ) -> ESMplusplusOutput:\n",
        "        \"\"\"Forward pass for token classification.\n",
        "\n",
        "        Args:\n",
        "            input_ids: Input token IDs\n",
        "            attention_mask: Attention mask\n",
        "            inputs_embeds: Optional precomputed embeddings\n",
        "            labels: Optional labels for token classification\n",
        "            output_hidden_states: Whether to return all hidden states\n",
        "            output_attentions: Whether to return attention weights\n",
        "\n",
        "        Returns:\n",
        "            ESMplusplusOutput containing loss, logits, and hidden states\n",
        "        \"\"\"\n",
        "        output = super().forward(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            labels=None,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states\n",
        "        )\n",
        "        x = output.last_hidden_state\n",
        "        logits = self.classifier(x)\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss = self.loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "\n",
        "        return ESMplusplusOutput(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            last_hidden_state=x,\n",
        "            hidden_states=output.hidden_states,\n",
        "        )\n"
      ],
      "metadata": {
        "id": "U1oWvONxJyUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading from EvolutionaryScale ESMplusplus models and sequence tokenizer"
      ],
      "metadata": {
        "id": "QeK2VZAZKNIP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Loading from EvolutionaryScale\n",
        "@staticmethod\n",
        "@cache\n",
        "def data_root(model: str):\n",
        "    if \"INFRA_PROVIDER\" in os.environ:\n",
        "        return Path(\"\")\n",
        "    # Try to download from hugginface if it doesn't exist\n",
        "    if model.startswith(\"esmc-300\"):\n",
        "        path = Path(snapshot_download(repo_id=\"EvolutionaryScale/esmc-300m-2024-12\"))\n",
        "    elif model.startswith(\"esmc-600\"):\n",
        "        path = Path(snapshot_download(repo_id=\"EvolutionaryScale/esmc-600m-2024-12\"))\n",
        "    else:\n",
        "        raise ValueError(f\"{model=} is an invalid model name.\")\n",
        "    return path\n",
        "\n",
        "\n",
        "def ESMplusplus_300M(device: torch.device | str = \"cpu\", num_labels: int = 3):\n",
        "    with torch.device(device):\n",
        "        config = ESMplusplusConfig(\n",
        "            hidden_size=960,\n",
        "            num_attention_heads=15,\n",
        "            num_hidden_layers=30,\n",
        "            num_labels=num_labels,\n",
        "        )\n",
        "        model = ESMplusplusForMaskedLM(config)\n",
        "    state_dict = torch.load(\n",
        "        data_root(\"esmc-300\") / \"data/weights/esmc_300m_2024_12_v0.pth\",\n",
        "        map_location=device,\n",
        "    )\n",
        "    model.load_state_dict(state_dict)\n",
        "    return model\n",
        "\n",
        "\n",
        "def ESMplusplus_600M(device: torch.device | str = \"cpu\", num_labels: int = 3):\n",
        "    with torch.device(device):\n",
        "        config = ESMplusplusConfig(\n",
        "            hidden_size=1152,\n",
        "            num_attention_heads=18,\n",
        "            num_hidden_layers=36,\n",
        "            num_labels=num_labels,\n",
        "        )\n",
        "        model = ESMplusplusForMaskedLM(config)\n",
        "        # # Use ESMplusplusForSequenceClassification to create the model with classification head\n",
        "        # model = ESMplusplusForSequenceClassification(config)\n",
        "    state_dict = torch.load(\n",
        "        data_root(\"esmc-600\") / \"data/weights/esmc_600m_2024_12_v0.pth\",\n",
        "        map_location=device,\n",
        "    )\n",
        "    model.load_state_dict(state_dict)\n",
        "    return model\n",
        "\n",
        "\n",
        "### Tokenization\n",
        "SEQUENCE_VOCAB = [\n",
        "    \"<cls>\", \"<pad>\", \"<eos>\", \"<unk>\",\n",
        "    \"L\", \"A\", \"G\", \"V\", \"S\", \"E\", \"R\", \"T\", \"I\", \"D\", \"P\", \"K\",\n",
        "    \"Q\", \"N\", \"F\", \"Y\", \"M\", \"H\", \"W\", \"C\", \"X\", \"B\", \"U\", \"Z\",\n",
        "    \"O\", \".\", \"-\", \"|\",\n",
        "    \"<mask>\",\n",
        "]\n",
        "\n",
        "class EsmSequenceTokenizer(PreTrainedTokenizerFast):\n",
        "    model_input_names = [\"input_ids\", \"attention_mask\"]\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        unk_token=\"<unk>\",\n",
        "        cls_token=\"<cls>\",\n",
        "        pad_token=\"<pad>\",\n",
        "        mask_token=\"<mask>\",\n",
        "        eos_token=\"<eos>\",\n",
        "        chain_break_token=\"|\",\n",
        "        **kwargs,\n",
        "    ):\n",
        "        all_tokens = SEQUENCE_VOCAB\n",
        "        token_to_id = {tok: ind for ind, tok in enumerate(all_tokens)}\n",
        "\n",
        "        # a character-level tokenizer is the same as BPE with no token merges\n",
        "        bpe = BPE(token_to_id, merges=[], unk_token=unk_token)\n",
        "        tokenizer = Tokenizer(bpe)\n",
        "        special_tokens = [\n",
        "            cls_token,\n",
        "            pad_token,\n",
        "            mask_token,\n",
        "            eos_token,\n",
        "            chain_break_token,\n",
        "        ]\n",
        "        self.cb_token = chain_break_token\n",
        "        additional_special_tokens = [chain_break_token]\n",
        "\n",
        "        tokenizer.add_special_tokens(special_tokens)\n",
        "\n",
        "        # This is where we configure the automatic addition of special tokens when we call\n",
        "        # tokenizer(text, add_special_tokens=True). Note that you can also configure how two\n",
        "        # sequences are merged if you want.\n",
        "        tokenizer.post_processor = TemplateProcessing(  # type: ignore\n",
        "            single=\"<cls> $A <eos>\",\n",
        "            special_tokens=[\n",
        "                (\"<cls>\", tokenizer.token_to_id(\"<cls>\")),\n",
        "                (\"<eos>\", tokenizer.token_to_id(\"<eos>\")),\n",
        "            ],\n",
        "        )\n",
        "        super().__init__(\n",
        "            tokenizer_object=tokenizer,\n",
        "            unk_token=unk_token,\n",
        "            cls_token=cls_token,\n",
        "            pad_token=pad_token,\n",
        "            mask_token=mask_token,\n",
        "            eos_token=eos_token,\n",
        "            additional_special_tokens=additional_special_tokens,\n",
        "            **kwargs,\n",
        "        )\n",
        "\n",
        "    # These are a footgun, we never use the `bos` token anywhere so we're just overriding it here.\n",
        "    @property\n",
        "    def bos_token(self):\n",
        "        return self.cls_token\n",
        "\n",
        "    @property\n",
        "    def bos_token_id(self):\n",
        "        return self.cls_token_id\n",
        "\n",
        "    @property\n",
        "    def chain_break_token(self):\n",
        "        return self.cb_token\n",
        "\n",
        "    @property\n",
        "    def chain_break_token_id(self):\n",
        "        return self.convert_tokens_to_ids(self.chain_break_token)\n",
        "\n",
        "    @property\n",
        "    def all_token_ids(self):\n",
        "        return list(range(self.vocab_size))\n",
        "\n",
        "    @property\n",
        "    def special_token_ids(self):\n",
        "        return self.all_special_ids\n"
      ],
      "metadata": {
        "id": "mITUidZzKNQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# My protein input data\n",
        "Formatted as required in the notebook `clean_myprot_PT5_LoRA_Finetuning_per_prot.ipynb`, I will modify it accordingly here."
      ],
      "metadata": {
        "id": "9rHgKC2d_JCp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Bio"
      ],
      "metadata": {
        "collapsed": true,
        "id": "FomD5CSp-wdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from Bio import SeqIO\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "input_seqs_path = \"/content/drive/My Drive/LLMs_data/input_seq/extracted_polymer_deg_proteins_1241_metagenomes_pident50_len550_final_format.tsv\"\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Using device: {}\".format(device))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVNgTvC3-waE",
        "outputId": "1c6ac8f5-291e-4062-ca58-3ccba6e40adc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Using device: cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3-ways split into train, test, val sets with each class distribution\n",
        "\n",
        "def stratified_3_split(df, target_col=\"target\", train_size=0.7, val_size=0.15, test_size=0.15, random_state=42):\n",
        "    \"\"\"\n",
        "    Splits a DataFrame into train, validation, and test sets while preserving class distribution.\n",
        "\n",
        "    Args:\n",
        "    - df (pd.DataFrame): The input DataFrame containing features and target labels.\n",
        "    - target_col (str): The column name containing the class labels.\n",
        "    - train_size (float): Proportion of data to allocate for training.\n",
        "    - val_size (float): Proportion of data to allocate for validation.\n",
        "    - test_size (float): Proportion of data to allocate for testing.\n",
        "    - random_state (int): Random seed for reproducibility.\n",
        "\n",
        "    Returns:\n",
        "    - train_df (pd.DataFrame): Training dataset.\n",
        "    - val_df (pd.DataFrame): Validation dataset.\n",
        "    - test_df (pd.DataFrame): Test dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    # Ensure proportions sum to 1\n",
        "    assert train_size + val_size + test_size == 1, \"Train, val, and test sizes must sum to 1.\"\n",
        "\n",
        "    # Extract features and labels\n",
        "    X = df.drop(columns=[target_col])\n",
        "    y = df[target_col]\n",
        "\n",
        "    # First, split into train (train_size) and temp (val_size + test_size)\n",
        "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "        X, y, test_size=(val_size + test_size), stratify=y, random_state=random_state\n",
        "    )\n",
        "\n",
        "    # Then, split temp into validation and test sets\n",
        "    X_val, X_test, y_val, y_test = train_test_split(\n",
        "        X_temp, y_temp, test_size=(test_size / (val_size + test_size)), stratify=y_temp, random_state=random_state\n",
        "    )\n",
        "\n",
        "    # Reassemble into DataFrames\n",
        "    train_df = X_train.copy()\n",
        "    train_df[target_col] = y_train\n",
        "    val_df = X_val.copy()\n",
        "    val_df[target_col] = y_val\n",
        "    test_df = X_test.copy()\n",
        "    test_df[target_col] = y_test\n",
        "\n",
        "    return train_df, val_df, test_df"
      ],
      "metadata": {
        "cellView": "form",
        "id": "NZHk6-zqAQ5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_for_llm = pd.read_csv(input_seqs_path, sep=\"\\t\")\n",
        "print(data_for_llm.shape)\n",
        "data_for_llm.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "m67LYoE6AYWk",
        "outputId": "8d302b57-ec66-4e03-fceb-355c5dfbef2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4367, 3)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 query                                           sequence  \\\n",
              "0  JACDJC010024513.1_3  MFRFHIVSALLTLFIAVPSQAHDVGQREIKISGAEPGRNLEVSVWY...   \n",
              "1  JAHZTL010540190.1_2  MRAWWLSGALALMFWAQGAVAGTLLVVGDSISAAFGLDSRQGWVAL...   \n",
              "2  JAHZTT010011795.1_8  MRAWWLSGALALMFWAQGAVAGTLLVVGDSISAAFGLDSRQGWVAL...   \n",
              "3  JANPXX010012215.1_8  MRAWWLSGALALMFWAQGAVAGTLLVVGDSISAAFGLDSRQGWVAL...   \n",
              "4   LAZR01000025.1_143  MQFLLGLIGLLLLIVTSLRRWLLRRESPQKQAVDFHGELYQVGSAV...   \n",
              "\n",
              "   label  \n",
              "0      2  \n",
              "1      0  \n",
              "2      0  \n",
              "3      0  \n",
              "4      1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a02c9ab3-3286-46cd-ba66-8ab264e132b9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>query</th>\n",
              "      <th>sequence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>JACDJC010024513.1_3</td>\n",
              "      <td>MFRFHIVSALLTLFIAVPSQAHDVGQREIKISGAEPGRNLEVSVWY...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>JAHZTL010540190.1_2</td>\n",
              "      <td>MRAWWLSGALALMFWAQGAVAGTLLVVGDSISAAFGLDSRQGWVAL...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>JAHZTT010011795.1_8</td>\n",
              "      <td>MRAWWLSGALALMFWAQGAVAGTLLVVGDSISAAFGLDSRQGWVAL...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>JANPXX010012215.1_8</td>\n",
              "      <td>MRAWWLSGALALMFWAQGAVAGTLLVVGDSISAAFGLDSRQGWVAL...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>LAZR01000025.1_143</td>\n",
              "      <td>MQFLLGLIGLLLLIVTSLRRWLLRRESPQKQAVDFHGELYQVGSAV...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a02c9ab3-3286-46cd-ba66-8ab264e132b9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a02c9ab3-3286-46cd-ba66-8ab264e132b9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a02c9ab3-3286-46cd-ba66-8ab264e132b9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-17356a02-3991-4ff7-a85a-6a21c020fa6e\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-17356a02-3991-4ff7-a85a-6a21c020fa6e')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-17356a02-3991-4ff7-a85a-6a21c020fa6e button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data_for_llm",
              "summary": "{\n  \"name\": \"data_for_llm\",\n  \"rows\": 4367,\n  \"fields\": [\n    {\n      \"column\": \"query\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4367,\n        \"samples\": [\n          \"JAWMQX010851707.1_1\",\n          \"JAWMQY011396312.1_2\",\n          \"JAUJDY010000031.1_21\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sequence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3340,\n        \"samples\": [\n          \"MSEPLILQPTLAADSCVIWLHGLGADRYDFMPVAEALQQRLHSTRFVLPQAPTQAVTINGGYAMPSWYDILAMSPARAINRDQLEASAQQVIALIEAQRDSGIDPARIVLAGFSQGGAVVLHTAFLRWQFALGGVMALSTYAPTFSDENTLADTKKQLPVLCLHGTFDDIVLPAMGRAAHDYLEASGVSVQWRDYPMGHEVVNEEIRDIADWLELRFNS*\",\n          \"MKKINHLFFLLACICSLNTTYAQQNIIEVEGGKISGVMNNYKTVESFKGIPFAAPPVGDLRWRAPQPVQPWNGVLACTKFSASPMQAKPVPFSMWSEEFLIPAEPISEDCLYLNVWTANKNKKNKQPVLVWIYGGGFGSGGTACPIYDGEALAKKGIVVVSINYRVGVFGFFAHPDLNEQSGNFGMLDQIAALKWVKKNIAAFGGDPDQVTISGQSAGSMSVNTLVASPLAAGLFNKAIAQSGGNFSRGNSSKSNAEAEGLKYAALFSAKTVAELKKVDAELLMKKFIGIRGPYIDGHVLPEHILDIFQKGNQNKVALLVGWNQDEGLMMSPAKSAENLRKDFQQQYGSNADAFLKFYPSSNDEEAKQTQLDLSRDQIFGMPGLIWANFQEAQSLPVYVYRFTRIVPAEGQYKQYKAFHTGEVPYMFDNLRFVRRPWEPADHELAKSMSDYWVSFVKTGNPNHSKALNWPLFNSKEKPTLYFDSKNKVAPMEDAERLNYLFSSMTANK*\",\n          \"MNTLSWIRSVNGTLGHLAPEHVARKMRRAFMTPRNRPPRDWELPLLARAERITLRFGLSALRWGQGPTVLLMHGWEGRPTQFAHLIDSLVDAGYTAVALEGPAHGHSPGNEANVVLFARALLEAAAELPPLKAVVGHSMGGASMLLALQWGLRAEVAVSIAAPAQLLGVIRGFARHLGMPARARAAFIRQIERDVGVQISRLDVSGYQLELPGLIVHAEDDQLVPVDESDAIHRAWFDSRLLRLPDGGHLRVLADPQLREGVLALLQRSSSPARQSA*\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 2,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          2,\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Need to remove duplicate sequences first in order to proceed with modeling**"
      ],
      "metadata": {
        "id": "xNgiR4SR1oCd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_for_llm = data_for_llm.drop_duplicates(subset=['sequence'], keep='first')\n",
        "print(data_for_llm.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMKZ1z8N1oPl",
        "outputId": "f722bcb5-effb-438c-8c8e-9a2f78e0e1ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3340, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate the length of each sequence and MAX:\n",
        "np.max(data_for_llm['sequence'].apply(len))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3UDbJejDce7",
        "outputId": "6a4e7893-a769-4b2a-bd2e-33082ab9ff6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "550"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_train, my_valid, my_test = stratified_3_split(data_for_llm, target_col=\"label\")\n",
        "my_train = my_train[['sequence', 'label']]\n",
        "my_valid = my_valid[['sequence', 'label']]\n",
        "my_test = my_test[['sequence', 'label']]\n",
        "print(f'The size of each split: train_df={my_train.shape[0]}, val_df={my_valid.shape[0]}, test_df={my_test.shape[0]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MnQG7X5IAYZ2",
        "outputId": "85c1e51a-7942-4942-8119-79b99e268c4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The size of each split: train_df=2338, val_df=501, test_df=501\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Base code for fine-tuning ESM++ with LORA for specific classification task"
      ],
      "metadata": {
        "id": "_IRCRaWZKeb6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets evaluate"
      ],
      "metadata": {
        "collapsed": true,
        "id": "iYPhay7bBVo-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "print(transformers.__version__)\n",
        "from torch.utils.data import TensorDataset # TensorDataset is a PyTorch class that creates a dataset from tensors (embeddings + labels). It provides a way to access your data in pairs of (embedding, label) for training or evaluation.\n",
        "from transformers import TrainingArguments, Trainer, AutoModelForMaskedLM\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "from datasets import load_dataset, Dataset\n",
        "from evaluate import load\n"
      ],
      "metadata": {
        "id": "V2yHs2HqGfuW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4922ec3c-6a46-4175-c0d3-65ebc3b9539d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.47.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ESM++ models from EvolutionaryScale are not directly hosted on the Hugging Face Model Hub in the standard format expected by from_pretrained. They are provided in a different format within their own repository. Use the custom loading mechanism defined in the notebook itself.\n",
        "\n",
        "__Warning note: the pretrained model does not contain weights for the classification head.__\n",
        "\n",
        "_The original warning message:\n",
        "Some weights of ESMplusplusForSequenceClassification were not initialized from the model checkpoint at Synthyra/ESMplusplus_large and are newly initialized: ['classifier.0.bias', 'classifier.0.weight', 'classifier.2.bias', 'classifier.2.weight', 'classifier.3.bias', 'classifier.3.weight']\n",
        "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference._\n",
        "\n",
        "__Note:\n",
        "strict=False:__ This is a crucial argument. It tells load_state_dict() to ignore any keys (parameters) in the state dictionary that are not found in the model_classification. Why is this important for your code? The model_embedding (ESM++ for MLM) has parameters for its backbone (embedding and transformer layers). The model_classification (ESM++ for sequence classification) has those same backbone parameters plus additional parameters for its classification head. By using strict=False, you can copy the shared backbone parameters from the pretrained model_embedding into the model_classification, while leaving the classification head's parameters untouched (they'll be initialized randomly or with some default). This enables transfer learning. You're taking the knowledge learned by the embedding model (about protein representations) and using it to initialize the backbone of the classification model."
      ],
      "metadata": {
        "id": "jm2lBb9VPLPu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ESM++ for protein embeddings using a pre-trained model from Synthyra\n",
        "\n",
        "model = AutoModelForMaskedLM.from_pretrained('Synthyra/ESMplusplus_large', trust_remote_code=True)\n",
        "tokenizer = model.tokenizer\n",
        "\n",
        "# Move model to GPU and keep them in float32\n",
        "model = model.to(device)\n",
        "\n",
        "# embeddings = model.embed_dataset(\n",
        "#     sequences=list(my_valid['sequence']), # list of protein strings\n",
        "#     batch_size=16, # embedding batch size\n",
        "#     max_len=550, # truncate to max_len\n",
        "#     full_embeddings=False, # return full residue-wise (True) embeddings or pooled (False)\n",
        "#     full_precision=True, # store as float32\n",
        "#     pooling_type='mean', # use mean pooling if protein-wise embeddings\n",
        "#     num_workers=0, # data loading num workers\n",
        "#     sql=False, # return dictionary of sequences and embeddings\n",
        "# )\n",
        "\n",
        "\n",
        "# Dataset creation\n",
        "def create_dataset(tokenizer,seqs,labels, padding=\"longest\", truncation=True, max_length=550):\n",
        "    tokenized = tokenizer(seqs, max_length=max_length, padding=padding, truncation=truncation)\n",
        "    dataset = Dataset.from_dict(tokenized)\n",
        "    dataset = dataset.add_column(\"labels\", labels)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "# Create Datasets\n",
        "train_set=create_dataset(tokenizer,list(my_train['sequence'][0:20]),list(my_train['label'][0:20]))\n",
        "valid_set=create_dataset(tokenizer,list(my_valid['sequence'][0:10]),list(my_valid['label'][0:10]))\n",
        "test_set=create_dataset(tokenizer,list(my_test['sequence']),list(my_test['label']))"
      ],
      "metadata": {
        "id": "rh1g2TUjgguV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_set)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjhRMoA6ry3n",
        "outputId": "652d1252-9ab1-4996-9b2e-b833358fa027"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['input_ids', 'attention_mask', 'labels'],\n",
            "    num_rows: 20\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a ProteinDataset instance with your protein sequences\n",
        "train_dataset = ProteinDataset(list(my_train['sequence']))\n",
        "test_dataset = ProteinDataset(list(my_test['sequence']))\n",
        "valid_dataset = ProteinDataset(list(my_valid['sequence']))\n",
        "\n",
        "# Initialize the model for protein embedding\n",
        "# Tokenizer is already initialized in this class (self.tokenizer = EsmSequenceTokenizer())\n",
        "model_embedding = ESMplusplus_600M(num_labels=3)\n",
        "tokenizer = model_embedding.tokenizer\n",
        "\n",
        "# Create a new ESMplusplusForSequenceClassification model\n",
        "# This model is separate from the model_embedding\n",
        "model_classification = ESMplusplusForSequenceClassification.from_pretrained_esm(\"600\")\n",
        "\n",
        "# Load the state dict of the base model (without the classification head) to the new model\n",
        "#model_classification.load_state_dict(model_embedding.state_dict(), strict=False)\n",
        "\n",
        "# # Move models to GPU and convert to half-precision (FP16)\n",
        "# model_embedding = model_embedding.to(device).half()\n",
        "# model_classification = model_classification.to(device).half()\n",
        "\n",
        "# Move models to GPU and keep them in float32\n",
        "model_embedding = model_embedding.to(device)  # Remove .half()\n",
        "model_classification = model_classification.to(device)  # Remove .half()\n",
        "\n",
        "\n",
        "def get_classification_dataset(model, dataset, labels, max_len=550, batch_size=16, pooling_type=\"mean\"):\n",
        "    \"\"\"Generate embeddings and create classification dataset.\n",
        "       The internal DataLoader within embed_dataset is already handling the batching of data during embedding generation.\n",
        "    \"\"\"\n",
        "    embeddings_dict = model.embed_dataset(\n",
        "        dataset,\n",
        "        max_len=max_len,\n",
        "        batch_size=batch_size,\n",
        "        pooling_type=pooling_type,\n",
        "        full_precision=True,\n",
        "    )\n",
        "\n",
        "    # Create a dictionary to store data\n",
        "    dataset_dict = {'inputs_embeds': [], 'labels': []}\n",
        "    for i in range(len(dataset)):  # Iterate by index\n",
        "        sequence = dataset[i]  # Get sequence using __getitem__\n",
        "        embedding = embeddings_dict.get(sequence)\n",
        "        if embedding is not None:\n",
        "            dataset_dict['inputs_embeds'].append(embedding)  # Append to list\n",
        "            dataset_dict['labels'].append(labels[i])       # Append to list\n",
        "        else:\n",
        "            print(f\"Warning: Sequence '{sequence}' not found in embeddings_dict. Skipping...\")\n",
        "\n",
        "    return Dataset.from_dict(dataset_dict)\n",
        "\n",
        "\n",
        "# Create TensorDatasets with embeddings and labels using get_classification_dataset\n",
        "train_dataset_classification = get_classification_dataset(model_embedding, list(my_train['sequence']), list(my_train['label']))\n",
        "test_dataset_classification = get_classification_dataset(model_embedding, list(my_test['sequence']), list(my_test['label']))\n",
        "valid_dataset_classification = get_classification_dataset(model_embedding, list(my_valid['sequence']), list(my_valid['label']))\n",
        "\n",
        "# # Embed the input sequences\n",
        "# train_embeddings = model_embedding.embed_dataset(train_dataset, max_len=550, batch_size=16, pooling_type=\"mean\")\n",
        "# test_embeddings = model_embedding.embed_dataset(test_dataset, max_len=550, batch_size=16, pooling_type=\"mean\")\n",
        "# valid_embeddings = model_embedding.embed_dataset(valid_dataset, max_len=550, batch_size=16, pooling_type=\"mean\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "T6L6oYMwLSjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_dataset_classification)\n",
        "print(train_dataset_classification[0])\n",
        "print(train_dataset_classification.features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lY7iCMG51i3j",
        "outputId": "c4e1694d-dea8-4ce8-f590-6453031ef8dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['inputs_embeds', 'labels'],\n",
            "    num_rows: 2338\n",
            "})\n",
            "{'inputs_embeds': [-0.023973623290657997, -0.04354223236441612, -0.004743195604532957, -0.02861812897026539, -0.0008747765677981079, -0.03435741364955902, 0.025471772998571396, -0.006187583319842815, 0.007463967427611351, 0.029759440571069717, -0.0021257500629872084, 0.018827270716428757, -0.012098248116672039, -0.02671876549720764, 0.03714583441615105, -0.031489357352256775, -0.0013180127134546638, -0.002125516999512911, 0.016997428610920906, 0.05265428125858307, 0.064813531935215, -0.00753829488530755, -0.011604984290897846, 0.014580518007278442, -0.012080320157110691, -0.03716570883989334, 0.027980389073491096, 0.006632362492382526, 0.038738857954740524, -0.0024991247337311506, 0.028428370133042336, 0.020869024097919464, -0.042663101106882095, 0.02030995674431324, -0.04061656817793846, 0.03700200095772743, -0.01907176710665226, -0.031393397599458694, 0.026571379974484444, -0.012965292669832706, -0.040591657161712646, 0.023525603115558624, -0.04455012083053589, -0.0008917442173697054, 0.012737688608467579, 0.020247407257556915, -0.029686937108635902, 0.03128818795084953, 0.0039086295291781425, -0.043375492095947266, -0.015604847110807896, -0.009442339651286602, -0.015126640908420086, -0.017832348123192787, -0.04110534489154816, 0.018219925463199615, -0.05925826355814934, 0.02533515729010105, -0.048445433378219604, 0.0034390429500490427, 0.05478684604167938, -0.057778045535087585, 0.012432430870831013, -0.005157222039997578, -0.0034574647434055805, -0.04550377279520035, 0.004863468464463949, -0.015631239861249924, -0.023836057633161545, -0.04316774383187294, -0.005634769797325134, -0.04648671671748161, 0.03114415891468525, 0.018491797149181366, -0.02680835872888565, -0.00011864653060911223, 0.01315809041261673, -0.060485392808914185, 0.020106598734855652, -0.0018057883717119694, -0.03196564316749573, -0.024616604670882225, -0.04350772500038147, 0.03163086995482445, -0.0016316327964887023, 0.012228625826537609, 0.05046107620000839, 0.015843015164136887, 0.03272391855716705, 0.02662699855864048, 0.0025428871158510447, -0.00337977334856987, -0.004214993212372065, -0.01760319620370865, 0.007710867561399937, -0.01200437918305397, -0.018767692148685455, -0.0005905897123739123, -0.007958881556987762, -0.0169216301292181, -0.009817484766244888, 0.0015560680767521262, 0.05704677477478981, -0.024747334420681, 0.0003018778224941343, 0.04563073813915253, -0.003029514802619815, 0.039037030190229416, 0.022840019315481186, 0.00041971521568484604, 0.007143460214138031, -0.042240992188453674, 0.026743410155177116, -0.0025893589481711388, 0.04325896129012108, -0.015415539965033531, -0.011749479919672012, 0.030453216284513474, 0.014997977763414383, -0.03148489072918892, 0.03128346800804138, 0.010392285883426666, -0.06271106004714966, 0.032006435096263885, -0.040801968425512314, 0.06614267826080322, -0.0361887663602829, -0.02842567302286625, 0.032584961503744125, 0.0018492278177291155, 0.02026449702680111, -0.007835467346012592, -0.06074509769678116, -0.004740296397358179, 0.040081724524497986, 0.006379701662808657, -0.019795846194028854, -0.020535672083497047, 0.002816807711496949, -0.0028539958875626326, -0.009448166005313396, 0.03134763240814209, 0.0071585820987820625, 0.018665190786123276, 0.02161649987101555, 0.004259816836565733, 0.017890632152557373, 0.022032175213098526, 0.025721745565533638, 0.04598185047507286, 0.035377275198698044, -0.01736627146601677, -0.012500017881393433, 0.00261451187543571, 0.03632649779319763, 0.04260290041565895, 0.014339336194097996, 0.03841990977525711, 0.042886439710855484, -0.054653190076351166, 0.013011125847697258, 0.03350485488772392, 0.02960369363427162, 0.040511272847652435, -0.014092797413468361, 0.03213940188288689, -0.018170688301324844, 0.013115647248923779, 0.0016890981933102012, 0.026022881269454956, 0.017047308385372162, -0.033087871968746185, 0.029640529304742813, -0.0438602939248085, -0.015649795532226562, 0.00835887249559164, -0.07676597684621811, -0.0058646416291594505, 0.02679908648133278, 0.005596267059445381, -0.01608867570757866, -0.02379915677011013, -0.030625447630882263, -0.011315924115478992, 0.052355311810970306, 0.020973490551114082, -0.014414604753255844, 0.03618849068880081, -0.04536385089159012, -0.02789245918393135, 0.059661198407411575, -0.001041519339196384, 0.021286694332957268, -0.052303194999694824, 0.035360269248485565, -0.031198544427752495, 0.00234138872474432, -0.033122751861810684, 0.030996255576610565, 0.006328079383820295, -0.015200119465589523, -0.014975030906498432, -0.017693743109703064, -0.03108653426170349, -0.04151363670825958, -0.03909185901284218, -0.013163202442228794, 0.0007038245676085353, 0.05288698524236679, 0.02392852120101452, -0.006305878981947899, 0.03275733441114426, -0.00364787713624537, 0.010651573538780212, -0.02116873487830162, -0.04489947855472565, 0.002203436102718115, -0.008647358044981956, 0.01453996542841196, 0.053539201617240906, -2.5516676032566465e-05, -0.00617879768833518, -0.011827688664197922, 0.00795690342783928, 0.06923801451921463, -0.005398737732321024, 0.03625025600194931, -0.047608520835638046, 0.004053874872624874, 0.010030665434896946, -0.01677853614091873, -0.008561039343476295, -0.04410901293158531, 0.011972078122198582, 0.0063498192466795444, 0.017707746475934982, 0.17604243755340576, 0.03525400161743164, -0.008562407456338406, 0.022159529849886894, -0.006759922951459885, 0.01686241291463375, -0.037725601345300674, 0.004113573115319014, 0.017356712371110916, -0.005410326179116964, 0.07852350920438766, -0.015032628551125526, 0.0443497896194458, -0.007502807769924402, 0.029431581497192383, -0.0027908787596970797, -0.0390755794942379, -0.01679372414946556, -0.03599896654486656, 0.026460932567715645, 0.03184305503964424, -0.011652897112071514, 0.0031999705825001, -0.026266062632203102, -0.03020387329161167, -0.18084782361984253, -0.007948415353894234, 0.036137085407972336, 0.021086515858769417, 0.04076800122857094, -0.011302550323307514, -0.01882057636976242, 0.027844415977597237, -0.1004764512181282, 0.026986731216311455, -0.01683582179248333, 0.006627381779253483, -0.03556763753294945, -0.05359483137726784, -0.014131395146250725, 0.003140856511890888, -0.01433749683201313, -0.0050557320937514305, 0.01218421570956707, 0.0029067934956401587, -0.021375292912125587, -0.003932422026991844, 0.020818494260311127, -0.04310788959264755, 0.007682539988309145, -0.0050302171148359776, 0.051527928560972214, 0.048293337225914, 0.053629420697689056, 0.006344780325889587, 0.011124405078589916, -0.007744074333459139, 0.02765541337430477, 0.011540021747350693, 0.012381207197904587, -0.002330886200070381, -0.018674669787287712, -0.023303188383579254, -0.044315584003925323, -0.003481833962723613, -0.0485956072807312, 0.025834837928414345, 0.028344687074422836, 0.012416399084031582, 0.0005630278028547764, -0.010568835772573948, -0.010257896035909653, -0.030180178582668304, -0.023893723264336586, 0.011573472991585732, 0.007015497423708439, -0.02071811631321907, 0.06910243630409241, -0.04097632318735123, 0.008518507704138756, 0.02223980613052845, -0.00523431645706296, 0.03278154134750366, -0.019590508192777634, 0.05924297124147415, 0.007780413143336773, 0.018625669181346893, 0.01325775682926178, -0.0008911401964724064, 0.0018078626599162817, 0.018826737999916077, 0.009590629488229752, 0.04325929656624794, -0.0049257902428507805, 0.013800757005810738, 0.01662375032901764, 0.01523144543170929, -0.008361103013157845, -0.2709629237651825, 0.06234635040163994, 0.024753684177994728, 0.035840559750795364, -0.004322938621044159, -0.00860690325498581, -0.000716131879016757, 0.03567001596093178, 0.0007778526633046567, -0.0275861993432045, -0.029442118480801582, 0.04715867340564728, 0.02645498514175415, -0.022903846576809883, 0.037735722959041595, 0.026321779936552048, 0.04909004271030426, -0.005865999963134527, -0.05870605260133743, -0.009222929365932941, -0.00024973732070066035, -0.02255168929696083, -0.005401287693530321, -0.010418086312711239, 0.010762800462543964, -0.027429666370153427, -0.038373738527297974, -0.0032973913475871086, -0.04320386424660683, -0.017311496660113335, -0.04281824827194214, -0.04937693849205971, 0.04406668245792389, 0.01625128649175167, -0.0458570271730423, -0.010054011829197407, -0.03970583528280258, 0.031275905668735504, 0.002612328389659524, -0.06485394388437271, -0.026117730885744095, -0.06929808855056763, 0.003705065930262208, 0.02128412015736103, -0.02984755113720894, -0.031068360432982445, 0.04380737990140915, 0.001551617169752717, 0.03926197811961174, 0.044935472309589386, -0.006214262451976538, 0.03952967748045921, 0.01016779150813818, 0.013106376864016056, 0.006757017690688372, -0.019775250926613808, 0.005314420908689499, -0.013717989437282085, 0.00888094212859869, -0.010851350612938404, -0.038761310279369354, 0.006629688199609518, 0.0037024449557065964, -0.0165287833660841, -0.05080598592758179, -0.011839841492474079, 0.05574304983019829, 0.0048826769925653934, 0.03785315528512001, -0.028219399973750114, 0.01202201284468174, -0.0039513916708528996, -0.02421613782644272, 0.01466428767889738, 0.003549885470420122, -0.041186410933732986, -0.017695492133498192, 0.03200165182352066, 0.05570108816027641, 0.01769508421421051, 0.0012527527287602425, -0.032459601759910583, -0.026885470375418663, 0.025025688111782074, 0.005724795628339052, 0.019176194444298744, 0.020732756704092026, 0.003312344430014491, -0.019344372674822807, -0.032594963908195496, 0.002329441485926509, -0.013257261365652084, -2.5798264687182382e-05, -0.02343592792749405, -0.00216125906445086, 0.015419824048876762, 0.017699414864182472, -0.0159841887652874, 0.01330697163939476, 0.014806420542299747, -0.03169365972280502, -0.03020371124148369, 0.0014326028758659959, 0.005556293297559023, -0.024960314854979515, -0.015991000458598137, -0.01803545095026493, -0.042077161371707916, -0.007513884920626879, -0.06424591690301895, 0.05303895100951195, -0.002323445165529847, -0.028878889977931976, 0.0286577045917511, -0.04584740102291107, 0.01380923017859459, -0.02189750038087368, 0.012944416143000126, -0.003483092412352562, 0.024073971435427666, -0.03939829021692276, 0.0004791216051671654, 0.03353600203990936, -0.011886942200362682, -0.021967215463519096, -0.01606171578168869, -0.0156266987323761, -0.0242193341255188, 0.03740151971578598, -0.02225566655397415, 0.017997803166508675, -0.015775704756379128, -0.04518207907676697, 0.004857372958213091, 0.03933090716600418, -0.026580404490232468, -0.020028131082654, 0.003941805101931095, 0.009048380888998508, -0.03563997894525528, 0.027632195502519608, -0.025121506303548813, 0.0030145635828375816, 0.003082376206293702, -0.007294367998838425, -0.01223684661090374, 0.014498146250844002, -0.030442578718066216, 0.03253374993801117, 0.03209392726421356, 0.034141555428504944, 0.047588225454092026, -0.01379871554672718, -0.014881226234138012, -0.03134123235940933, 0.025794707238674164, -0.016653457656502724, 0.012134806253015995, 0.033258382230997086, 0.03115265816450119, -0.038366369903087616, 0.03762437030673027, 0.004636636935174465, -0.02336888574063778, 0.0006562418420799077, 0.07021112740039825, -0.07892638444900513, 0.010433591902256012, -0.05538038909435272, 0.015471682883799076, 0.002055748598650098, -0.010854851454496384, -0.04526381194591522, 0.05027053877711296, -0.02856741100549698, -0.011061768978834152, 0.016829146072268486, -0.005758127197623253, 0.015705585479736328, 0.041658952832221985, 0.02164786495268345, -0.017308248206973076, 0.007732804398983717, 0.01637152023613453, -0.021526753902435303, -0.0341588519513607, 0.03900088369846344, 0.01592150889337063, 0.5849787592887878, 0.042230214923620224, 0.016688788309693336, 0.04119541496038437, -0.03079276718199253, 0.007762022316455841, -0.011662864126265049, 0.0008916946244426072, -8.910949873097707e-06, 0.02595897950232029, -0.01470464188605547, -0.0022336672991514206, -0.01704993098974228, 0.003628841135650873, 0.04222901538014412, -0.004159162286669016, -0.0156182162463665, 0.0073975976556539536, 0.025306299328804016, 0.003222597297281027, 0.014064116403460503, -0.021870942786335945, 0.01729544810950756, 0.0007572364993393421, 0.05127537623047829, 0.059607893228530884, -0.03639751672744751, -0.014753326773643494, 0.00912599265575409, -0.014208542183041573, -0.018716728314757347, 0.04626050218939781, -0.019936438649892807, 0.007811823394149542, 0.03166019171476364, 0.02709132805466652, -0.009172237478196621, -0.00963586661964655, -0.052708592265844345, -0.01263118814677, -0.003456075908616185, 0.012289285659790039, -0.015012741088867188, 0.008851459249854088, 0.017267068848013878, 0.029963329434394836, 0.00047420768532902, 0.026256050914525986, 0.03741159290075302, 0.019846590235829353, -0.022232461720705032, 0.029431574046611786, 0.020626716315746307, 0.020258676260709763, -0.014017443172633648, -0.04678377881646156, -0.011400816030800343, 4.445626018423354e-06, 0.01706979051232338, 0.012076794169843197, 0.019223159179091454, 0.07930679619312286, -0.02814936637878418, -0.007645139936357737, 0.05513651296496391, -0.02632388100028038, -0.02838955447077751, 0.01858571544289589, 0.02549847401678562, -0.005423758178949356, -0.01802545227110386, 0.0331510528922081, -0.02622559852898121, 0.023655060678720474, -0.003723472822457552, 0.0636046826839447, -0.0004089715948794037, 0.030077440664172173, 0.03347160294651985, 0.02188972383737564, -0.04622003063559532, -0.016935225576162338, 0.0007255467935465276, 0.020362727344036102, 0.01975017786026001, 0.006859340704977512, 0.003022367600351572, -0.021359477192163467, 0.022110948339104652, 0.019687853753566742, 0.021781591698527336, -0.013459249399602413, -0.016252653673291206, 0.019441144540905952, 0.02012461982667446, 0.014956002123653889, -0.049934808164834976, 0.00958313513547182, -0.19567953050136566, 0.014786270447075367, -0.0017427459824830294, 0.030257878825068474, 0.014565489254891872, 0.011348698288202286, 0.01447734422981739, 0.016491739079356194, 0.04321734234690666, 0.0018378498498350382, 0.016010815277695656, 0.012926208786666393, -0.025988271459937096, 0.04128395766019821, 0.01849706843495369, -0.029416367411613464, -0.006865829695016146, 0.01949961483478546, -0.010071747936308384, 0.011870298534631729, 0.01028634887188673, -0.0011487490264698863, 0.023672543466091156, -0.010567864403128624, -0.010286327451467514, -0.0014201448066160083, 0.040764790028333664, -0.0006523560150526464, -0.038899846374988556, 0.026318401098251343, -0.012223649770021439, -0.016646219417452812, -0.011716609820723534, 0.02821139432489872, -0.04229817911982536, -0.008755193091928959, 0.005444489885121584, 0.03087877854704857, 0.0014287122758105397, 0.038775090128183365, 0.002231849357485771, 0.040440864861011505, 0.03294313699007034, -0.05312281474471092, -0.006943363230675459, -0.017111266031861305, -0.04059620574116707, 0.022526657208800316, 0.0004514606262091547, 0.038755789399147034, -0.031059356406331062, 0.0084476787596941, 0.05160956829786301, 0.019364936277270317, -0.03031667321920395, -0.02590988390147686, 0.008422000333666801, -0.32361501455307007, 0.020514724776148796, 0.03426327556371689, -0.009248756803572178, -0.013161811977624893, -0.02126474864780903, 0.01960570365190506, 0.03228825330734253, 0.0038165180012583733, -0.04764927178621292, -0.0009716283529996872, 0.03627150505781174, 0.011686781421303749, 0.013828120194375515, 0.04841027036309242, 0.007508070673793554, -0.04372008144855499, 0.005649607162922621, 0.03230924531817436, -0.029890665784478188, -0.009695455431938171, 0.012853407301008701, -0.09131466597318649, -0.07762934267520905, 0.006750680040568113, -0.026105651631951332, -0.03884639963507652, -0.03311648964881897, 0.038402117788791656, -0.03386060148477554, -0.0025266786105930805, -0.020983757451176643, -0.022414468228816986, 0.007444765884429216, -0.017505884170532227, -0.007691564504057169, -0.004863210022449493, 0.010121827013790607, -0.001610842882655561, -0.0010590573074296117, 0.05588860437273979, 0.03889573737978935, 0.01663653925061226, 0.02657599374651909, -0.011116011999547482, -0.03686125576496124, -0.04043738543987274, -0.014434795826673508, 0.023805126547813416, 0.01627282053232193, 0.03506908193230629, 0.008410381153225899, 0.013240628875792027, 0.045723553746938705, 0.025759253650903702, -0.02486218884587288, 0.03341927379369736, -0.010032028891146183, 0.01675988733768463, 0.0026196043472737074, 0.005688680801540613, 0.027530234307050705, -0.02308766357600689, 0.020969068631529808, 0.022477436810731888, -0.019536258652806282, 0.011212209239602089, 0.052612245082855225, -0.03164256736636162, -0.008906290866434574, -0.009342866018414497, 0.0054806857369840145, 0.03714854270219803, -0.029276898130774498, 0.009923345409333706, -0.01568700559437275, -0.022055763751268387, -0.02360576204955578, -0.011777290143072605, 0.019038300961256027, -0.02850697748363018, -0.0031858105212450027, 0.01173725537955761, 0.011649351567029953, 0.04692838340997696, -0.06453957408666611, 0.016174256801605225, 0.014300091192126274, 0.0068990085273981094, 0.031942836940288544, 0.025609400123357773, 0.018368050456047058, 0.03487544134259224, -0.04100165516138077, -0.03426915779709816, 0.04507110267877579, -0.046306855976581573, -0.07395292073488235, -0.02083737775683403, -0.025672506541013718, -0.03416996821761131, 0.010969165712594986, 0.004851249046623707, 0.021548617631196976, 0.047902025282382965, -0.02077464945614338, 0.036891836673021317, -0.0014251845423132181, 0.0006822425057180226, -0.008004937320947647, 0.003938587848097086, -0.02387510985136032, 0.029820455238223076, -0.02228289283812046, -0.005245001055300236, -0.029389692470431328, -0.016178322955965996, 0.03316792845726013, -0.004025291185826063, 0.0197225883603096, -0.048343028873205185, -0.029540028423070908, 0.051313575357198715, 0.00845952145755291, -0.026959434151649475, 0.0016672916244715452, -0.0017517078667879105, 0.026558928191661835, -0.005505539942532778, -0.012781010940670967, 0.005015529692173004, 0.0286096278578043, 0.03410100191831589, -0.01505734957754612, 0.002459297887980938, 0.0576263889670372, 0.005731699522584677, -0.07876075059175491, 0.01938977651298046, -0.03595894202589989, -0.0150035684928298, 0.014028646051883698, 0.05392136424779892, 0.01662265509366989, -0.06368608772754669, 0.029401736333966255, -0.00490760849788785, 0.009209553711116314, 0.0026290134992450476, 0.03581619635224342, -0.022255491465330124, 0.0021097722928971052, 0.03437979146838188, 0.04104302451014519, 0.0045355199836194515, 0.019037630409002304, -0.015535612590610981, -0.0017318016616627574, 0.004925084766000509, -0.04471435397863388, -0.03950436785817146, -0.05145666375756264, -0.02162005938589573, 0.018631819635629654, -0.0007069007842801511, -0.009272301569581032, -0.037322308868169785, -0.010559587739408016, 0.034288808703422546, 0.05709151178598404, 0.01976787857711315, 0.021612651646137238, 0.04004841297864914, -0.013262446038424969, 0.007340926676988602, -0.0005808819551020861, 0.0024510992225259542, 0.009901664219796658, -0.026996320113539696, -0.06430035084486008, -0.03257068619132042, 0.0456569567322731, -0.041184037923812866, -0.004956224001944065, 0.029956502839922905, 0.023300260305404663, 0.012993195094168186, -0.035353876650333405, -0.003340037539601326, 0.012228980660438538, -0.03906191885471344, 0.03437735140323639, -0.0032704828772693872, 0.0010339318541809916, -0.03670547902584076, -0.031583741307258606, 0.032694261521101, -0.02509654127061367, -0.0705702155828476, 0.021271320059895515, 0.009033841080963612, -0.01518265437334776, -0.018947172909975052, 0.020485781133174896, -0.010678475722670555, -0.01834227330982685, -0.01125081442296505, 0.027233529835939407, 0.021741094067692757, -0.020874354988336563, -0.0038344745989888906, -0.020079491659998894, -0.02177288569509983, 0.02795347198843956, -0.042136285454034805, 0.011559301987290382, 0.020352916792035103, -0.01311791967600584, -0.018280521035194397, -0.010344013571739197, 0.0480266809463501, 0.005058169364929199, 0.029975226148962975, -0.008081096224486828, -0.013777952641248703, 0.014439522288739681, -0.014110791496932507, 0.0009853871306404471, 0.009602290578186512, -0.0020120213739573956, -0.011492712423205376, -0.000574537378270179, 0.016666285693645477, -0.011231141164898872, 0.028177909553050995, -0.00681541720405221, -0.014283396303653717, -0.06198517978191376, -0.009745487943291664, -0.029215088114142418, 0.03095434419810772, -0.005921740550547838, 0.011342236772179604, -0.0445048063993454, 0.006441878620535135, -0.026860198006033897, -0.06961475312709808, -0.019663603976368904, 0.002533563179895282, 0.008486277423799038, -0.030569737777113914, -0.03321775421500206, -0.02255064621567726, -0.04656016826629639, -0.032875675708055496, -0.0020296149887144566, 0.020167263224720955, 0.0786314532160759, -0.03445008024573326, 0.06457135826349258, -0.03480733931064606, 3.4516764571890235e-05, -0.010103864595293999, -0.027071798220276833, 0.015424448996782303, 0.000896092620678246, 0.021517805755138397, -0.047077279537916183, -0.002919884165748954, -0.033006634563207626, -0.01335968729108572, 0.014618071727454662, 0.01335916668176651, -0.02101314440369606, 0.01718980073928833, 0.04226189851760864, 0.008210954256355762, 0.02065345272421837, 0.0002707040694076568, -0.00030837065423838794, 0.0003972587001044303, -0.014626999385654926, 0.025657866150140762, -0.004518548026680946, -0.04335002228617668, 0.040776655077934265, -0.031039681285619736, -0.011978771537542343, -0.02153605781495571, -0.029891692101955414, 0.018937043845653534, 0.025380317121744156, -0.05841286852955818, 0.004830196965485811, -0.003993515390902758, -0.013096517883241177, 0.033131130039691925, -0.02904759906232357, -0.026129310950636864, -0.04766958951950073, -0.018165793269872665, -0.01938312128186226, -0.02842460758984089, 0.027012428268790245, -0.03166933357715607, 0.018481917679309845, -0.010644454509019852, 0.05730433389544487, -0.01676296815276146, -0.03934285044670105, 0.010238098911941051, 0.026449698954820633, 0.01160301174968481, -0.03721311315894127, -0.0053704530000686646, -0.01718721352517605, -0.004725602921098471, -0.03383199870586395, -0.01707613095641136, 0.002331478288397193, 0.041667401790618896, 0.036399535834789276, -0.015159005299210548, 0.026551498100161552, -0.002655012533068657, 0.029905755072832108, -0.0016891679260879755, -0.028788305819034576, -0.018995217978954315, 0.015827642753720284, 0.0009096275898627937, 0.007499040104448795, -0.01790560595691204, 0.022538580000400543, 0.01232601422816515, 0.011388842016458511, -0.004553663078695536, -0.012597533874213696, -0.02358086220920086, 0.013204818591475487, -0.010098270140588284, -0.025065952911973, -0.024552442133426666, 0.029467588290572166, 0.0012794361682608724, 0.0046224892139434814, 0.008916337974369526, -0.07397076487541199, 0.04112093895673752, 0.009327073581516743, -0.019335508346557617, -0.022457562386989594, 0.025024887174367905, -0.031097494065761566, -0.021769938990473747, 0.016030021011829376, 0.04138363152742386, 0.00033323621028102934, -0.11951572448015213, 0.00762978894636035, -0.025578107684850693, 0.025653747841715813, 0.017824526876211166, -0.02421068400144577, 0.04065333306789398, 0.03304562345147133, -0.010767528787255287, -0.3930058181285858, 0.0020131065975874662, 0.009768493473529816, 0.004401803016662598, -0.03914091736078262, -0.031600482761859894, -0.06039272993803024, 0.011621948331594467, -0.03316529467701912, 0.007107970770448446, -0.016616424545645714, -0.012143618427217007, -0.01882796920835972, -0.02661745995283127, -0.0399741567671299, 0.02495506778359413, 0.009502533823251724, 0.023312775418162346, -0.01483589131385088, 0.03273526206612587, -0.011746464297175407, 0.0203620083630085, 0.03859797492623329, -0.015570750460028648, -0.0014653519028797746, 0.010848027653992176, 0.04695482552051544, 0.041920989751815796, -0.04635418951511383, -0.06674155592918396, -0.03166165575385094, -0.012961141765117645, -0.015750212594866753, -0.05342858284711838, 0.07412116974592209, 0.00988814327865839, 0.004782188218086958, 0.033013563603162766, 0.005712945479899645, 0.022408856078982353, 0.031799349933862686, -0.0010041710920631886, 0.004383146297186613, 0.014325694181025028, 0.015618163160979748, 0.030591599643230438, 0.006343426648527384, -0.6162088513374329, 0.041204165667295456, 0.052732646465301514, 0.0091498252004385, -0.0503961406648159, 0.05209236592054367, 0.007099459413439035, 0.02385171316564083, 0.026116035878658295, -0.023354800418019295, -0.02511952631175518, 0.013878540135920048, -0.018157297745347023, 0.004234577529132366, 0.036433927714824677, 0.0319741927087307, 0.005742316599935293, 0.008948140777647495, 0.01034616306424141, 0.004659814760088921, 0.005317851901054382, -0.010996158234775066, 0.05137130245566368, 0.01389306876808405, 0.059429530054330826, -0.012797628529369831, 0.004654861520975828, -0.026699671521782875, -0.030580995604395866, 0.05263639613986015, -0.015936782583594322, 0.0043012541718780994, -0.018004318699240685, -0.012355377897620201, -0.0027279979549348354, 0.026677042245864868, 0.022167973220348358, 0.09054021537303925, 0.009398876689374447, -0.0005096765235066414, -0.020921852439641953, -0.04232770577073097, 0.009609551168978214, 0.05235486850142479, -0.006603296846151352, -0.04333863779902458, -0.012249216437339783, 0.009194384329020977, 0.010159507393836975, 0.0005680583999492228, 0.03827515244483948, -0.002719202311709523, -0.003489011200144887, -0.057353466749191284, -0.006153381895273924, 0.06509438902139664, 0.007848657667636871, 0.003715143073350191, 0.014576158486306667, 0.015360998921096325, 0.0214821957051754], 'labels': 0}\n",
            "{'inputs_embeds': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), 'labels': Value(dtype='int64', id=None)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(list(train_dataset_classification[0].values())[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnhxAYJ6O5Fs",
        "outputId": "65f4af22-5e95-4459-c39c-2ee9352f0708"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1152\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Print the model's architecture__ to identify the layers in the model_classification that you want LoRA to adapt."
      ],
      "metadata": {
        "id": "Ff2Gb-pZHWfz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model_classification)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vN4AJMTR70Mp",
        "outputId": "affa6694-c4be-4d83-8ae2-c91ff51c1310"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ESMplusplusForMaskedLM(\n",
            "  (embed): Embedding(64, 1152)\n",
            "  (transformer): TransformerStack(\n",
            "    (blocks): ModuleList(\n",
            "      (0-35): 36 x UnifiedTransformerBlock(\n",
            "        (attn): MultiHeadAttention(\n",
            "          (layernorm_qkv): Sequential(\n",
            "            (0): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
            "            (1): Linear(in_features=1152, out_features=3456, bias=False)\n",
            "          )\n",
            "          (out_proj): Linear(in_features=1152, out_features=1152, bias=False)\n",
            "          (q_ln): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
            "          (k_ln): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
            "          (rotary): RotaryEmbedding()\n",
            "        )\n",
            "        (ffn): Sequential(\n",
            "          (0): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
            "          (1): Linear(in_features=1152, out_features=6144, bias=False)\n",
            "          (2): SwiGLU()\n",
            "          (3): Linear(in_features=3072, out_features=1152, bias=False)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (norm): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (sequence_head): Sequential(\n",
            "    (0): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "    (1): GELU(approximate='none')\n",
            "    (2): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
            "    (3): Linear(in_features=1152, out_features=64, bias=True)\n",
            "  )\n",
            "  (ce_loss): CrossEntropyLoss()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Using Regex Wildcards with LoRA to select target modules__\n",
        "\n",
        "While LoRA itself doesn't directly support regex wildcards in the target_modules parameter, you can achieve a similar effect by programmatically generating the list of target modules using regex matching."
      ],
      "metadata": {
        "id": "FQiNtai1Lj6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the regex pattern to match desired layers (excluding LayerNorm - ffn.0)\n",
        "pattern = r\"transformer\\.blocks\\.\\d+\\.(attn\\.layernorm_qkv\\.1|attn\\.out_proj|ffn\\.[13])\"\n",
        "\n",
        "\n",
        "# Get all matching module names\n",
        "target_modules = [\n",
        "    name\n",
        "    for name, module in model_classification.named_modules() # iterate through all modules and their names.\n",
        "    if re.fullmatch(pattern, name)\n",
        "]\n",
        "print(f'Target modules for LORA: {target_modules}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I29KrCvJLkGi",
        "outputId": "49a9876a-5087-4a58-d2c7-95c99ce381f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target modules for LORA: ['transformer.blocks.0.attn.layernorm_qkv.1', 'transformer.blocks.0.attn.out_proj', 'transformer.blocks.0.ffn.1', 'transformer.blocks.0.ffn.3', 'transformer.blocks.1.attn.layernorm_qkv.1', 'transformer.blocks.1.attn.out_proj', 'transformer.blocks.1.ffn.1', 'transformer.blocks.1.ffn.3', 'transformer.blocks.2.attn.layernorm_qkv.1', 'transformer.blocks.2.attn.out_proj', 'transformer.blocks.2.ffn.1', 'transformer.blocks.2.ffn.3', 'transformer.blocks.3.attn.layernorm_qkv.1', 'transformer.blocks.3.attn.out_proj', 'transformer.blocks.3.ffn.1', 'transformer.blocks.3.ffn.3', 'transformer.blocks.4.attn.layernorm_qkv.1', 'transformer.blocks.4.attn.out_proj', 'transformer.blocks.4.ffn.1', 'transformer.blocks.4.ffn.3', 'transformer.blocks.5.attn.layernorm_qkv.1', 'transformer.blocks.5.attn.out_proj', 'transformer.blocks.5.ffn.1', 'transformer.blocks.5.ffn.3', 'transformer.blocks.6.attn.layernorm_qkv.1', 'transformer.blocks.6.attn.out_proj', 'transformer.blocks.6.ffn.1', 'transformer.blocks.6.ffn.3', 'transformer.blocks.7.attn.layernorm_qkv.1', 'transformer.blocks.7.attn.out_proj', 'transformer.blocks.7.ffn.1', 'transformer.blocks.7.ffn.3', 'transformer.blocks.8.attn.layernorm_qkv.1', 'transformer.blocks.8.attn.out_proj', 'transformer.blocks.8.ffn.1', 'transformer.blocks.8.ffn.3', 'transformer.blocks.9.attn.layernorm_qkv.1', 'transformer.blocks.9.attn.out_proj', 'transformer.blocks.9.ffn.1', 'transformer.blocks.9.ffn.3', 'transformer.blocks.10.attn.layernorm_qkv.1', 'transformer.blocks.10.attn.out_proj', 'transformer.blocks.10.ffn.1', 'transformer.blocks.10.ffn.3', 'transformer.blocks.11.attn.layernorm_qkv.1', 'transformer.blocks.11.attn.out_proj', 'transformer.blocks.11.ffn.1', 'transformer.blocks.11.ffn.3', 'transformer.blocks.12.attn.layernorm_qkv.1', 'transformer.blocks.12.attn.out_proj', 'transformer.blocks.12.ffn.1', 'transformer.blocks.12.ffn.3', 'transformer.blocks.13.attn.layernorm_qkv.1', 'transformer.blocks.13.attn.out_proj', 'transformer.blocks.13.ffn.1', 'transformer.blocks.13.ffn.3', 'transformer.blocks.14.attn.layernorm_qkv.1', 'transformer.blocks.14.attn.out_proj', 'transformer.blocks.14.ffn.1', 'transformer.blocks.14.ffn.3', 'transformer.blocks.15.attn.layernorm_qkv.1', 'transformer.blocks.15.attn.out_proj', 'transformer.blocks.15.ffn.1', 'transformer.blocks.15.ffn.3', 'transformer.blocks.16.attn.layernorm_qkv.1', 'transformer.blocks.16.attn.out_proj', 'transformer.blocks.16.ffn.1', 'transformer.blocks.16.ffn.3', 'transformer.blocks.17.attn.layernorm_qkv.1', 'transformer.blocks.17.attn.out_proj', 'transformer.blocks.17.ffn.1', 'transformer.blocks.17.ffn.3', 'transformer.blocks.18.attn.layernorm_qkv.1', 'transformer.blocks.18.attn.out_proj', 'transformer.blocks.18.ffn.1', 'transformer.blocks.18.ffn.3', 'transformer.blocks.19.attn.layernorm_qkv.1', 'transformer.blocks.19.attn.out_proj', 'transformer.blocks.19.ffn.1', 'transformer.blocks.19.ffn.3', 'transformer.blocks.20.attn.layernorm_qkv.1', 'transformer.blocks.20.attn.out_proj', 'transformer.blocks.20.ffn.1', 'transformer.blocks.20.ffn.3', 'transformer.blocks.21.attn.layernorm_qkv.1', 'transformer.blocks.21.attn.out_proj', 'transformer.blocks.21.ffn.1', 'transformer.blocks.21.ffn.3', 'transformer.blocks.22.attn.layernorm_qkv.1', 'transformer.blocks.22.attn.out_proj', 'transformer.blocks.22.ffn.1', 'transformer.blocks.22.ffn.3', 'transformer.blocks.23.attn.layernorm_qkv.1', 'transformer.blocks.23.attn.out_proj', 'transformer.blocks.23.ffn.1', 'transformer.blocks.23.ffn.3', 'transformer.blocks.24.attn.layernorm_qkv.1', 'transformer.blocks.24.attn.out_proj', 'transformer.blocks.24.ffn.1', 'transformer.blocks.24.ffn.3', 'transformer.blocks.25.attn.layernorm_qkv.1', 'transformer.blocks.25.attn.out_proj', 'transformer.blocks.25.ffn.1', 'transformer.blocks.25.ffn.3', 'transformer.blocks.26.attn.layernorm_qkv.1', 'transformer.blocks.26.attn.out_proj', 'transformer.blocks.26.ffn.1', 'transformer.blocks.26.ffn.3', 'transformer.blocks.27.attn.layernorm_qkv.1', 'transformer.blocks.27.attn.out_proj', 'transformer.blocks.27.ffn.1', 'transformer.blocks.27.ffn.3', 'transformer.blocks.28.attn.layernorm_qkv.1', 'transformer.blocks.28.attn.out_proj', 'transformer.blocks.28.ffn.1', 'transformer.blocks.28.ffn.3', 'transformer.blocks.29.attn.layernorm_qkv.1', 'transformer.blocks.29.attn.out_proj', 'transformer.blocks.29.ffn.1', 'transformer.blocks.29.ffn.3', 'transformer.blocks.30.attn.layernorm_qkv.1', 'transformer.blocks.30.attn.out_proj', 'transformer.blocks.30.ffn.1', 'transformer.blocks.30.ffn.3', 'transformer.blocks.31.attn.layernorm_qkv.1', 'transformer.blocks.31.attn.out_proj', 'transformer.blocks.31.ffn.1', 'transformer.blocks.31.ffn.3', 'transformer.blocks.32.attn.layernorm_qkv.1', 'transformer.blocks.32.attn.out_proj', 'transformer.blocks.32.ffn.1', 'transformer.blocks.32.ffn.3', 'transformer.blocks.33.attn.layernorm_qkv.1', 'transformer.blocks.33.attn.out_proj', 'transformer.blocks.33.ffn.1', 'transformer.blocks.33.ffn.3', 'transformer.blocks.34.attn.layernorm_qkv.1', 'transformer.blocks.34.attn.out_proj', 'transformer.blocks.34.ffn.1', 'transformer.blocks.34.ffn.3', 'transformer.blocks.35.attn.layernorm_qkv.1', 'transformer.blocks.35.attn.out_proj', 'transformer.blocks.35.ffn.1', 'transformer.blocks.35.ffn.3']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LoRA configuration\n",
        "lora_config = LoraConfig(\n",
        "    r=4,  # Rank of the LoRA update matrices\n",
        "    lora_alpha=32,  # Scaling factor for the LoRA update matrices\n",
        "    lora_dropout=0.05,  # Dropout probability for the LoRA update matrices\n",
        "    bias=\"none\",  # Whether to apply bias to the LoRA update matrices\n",
        "    task_type=TaskType.SEQ_CLS,  # Task type for sequence classification\n",
        "    target_modules=target_modules,  # Modules which LORA method should target and modify their weights\n",
        ")\n",
        "\n",
        "# Apply LoRA to the classification model\n",
        "model = get_peft_model(model_classification, lora_config)\n",
        "\n",
        "# Prints the number of trainable parameters in the LoRA-adapted model\n",
        "model.print_trainable_parameters()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9-L4undMsDZ",
        "outputId": "10ea305e-0f52-4061-d05a-2fdecd455646"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 2,654,208 || all params: 577,617,472 || trainable%: 0.4595\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataCollator:\n",
        "    def __call__(self, features):\n",
        "        # Accessing embeddings and labels from Hugging Face Dataset\n",
        "        inputs_embeds = [f['inputs_embeds'] for f in features]\n",
        "        labels = [f['labels'] for f in features]\n",
        "\n",
        "        # Convert inputs_embeds elements to PyTorch tensors if necessary\n",
        "        inputs_embeds = [torch.tensor(x) if not isinstance(x, torch.Tensor) else x for x in inputs_embeds]\n",
        "        inputs_embeds = [x.unsqueeze(0) for x in inputs_embeds] # Add sequence length dimension\n",
        "\n",
        "        # Convert the lists to tensors and create batch\n",
        "        batch = {\n",
        "            \"inputs_embeds\": torch.stack(inputs_embeds),  # Stack inputs_embeds to add batch dimension\n",
        "            \"labels\": torch.tensor(labels),\n",
        "        }\n",
        "        return batch\n",
        "\n",
        "data_collator = CustomDataCollator()"
      ],
      "metadata": {
        "id": "tZhIMNJqSvO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Testing the custom data collator on a small batch of embeddings to check their dimensions\n",
        "\n",
        "test_batch = [train_dataset_classification[i] for i in range(5)]\n",
        "data_collator = CustomDataCollator()\n",
        "\n",
        "# Convert inputs_embeds elements to PyTorch tensors\n",
        "for sample in test_batch:\n",
        "    sample['inputs_embeds'] = torch.tensor(sample['inputs_embeds'])\n",
        "    print(sample)\n",
        "\n",
        "collated_batch = data_collator(test_batch)\n",
        "print(collated_batch['inputs_embeds'].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9G0ZNmuV2GZ",
        "outputId": "fa93bc7c-9400-4aa3-b9b9-073f2bd3c5d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'inputs_embeds': tensor([-0.0240, -0.0435, -0.0047,  ...,  0.0146,  0.0154,  0.0215]), 'labels': 0}\n",
            "{'inputs_embeds': tensor([-0.0348, -0.0418, -0.0068,  ...,  0.0146,  0.0205,  0.0159]), 'labels': 0}\n",
            "{'inputs_embeds': tensor([-0.0559, -0.0219, -0.0123,  ...,  0.0142, -0.0040,  0.0055]), 'labels': 0}\n",
            "{'inputs_embeds': tensor([-0.0571, -0.0342, -0.0193,  ...,  0.0175,  0.0003,  0.0211]), 'labels': 0}\n",
            "{'inputs_embeds': tensor([-0.0310, -0.0395, -0.0089,  ...,  0.0115,  0.0090,  0.0215]), 'labels': 2}\n",
            "torch.Size([5, 1, 1152])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Huggingface Trainer arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    eval_strategy = \"epoch\",\n",
        "    logging_strategy = \"epoch\",\n",
        "    save_strategy = \"epoch\",\n",
        "    learning_rate=3e-4,\n",
        "    # effective training batch size is batch * accum\n",
        "    # we recommend an effective training batch size of 8\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=16,\n",
        "    gradient_accumulation_steps=2,\n",
        "    num_train_epochs=10,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    #deepspeed= ds_config if deepspeed else None,\n",
        "    fp16 = False,\n",
        "    gradient_checkpointing=False,\n",
        ")\n",
        "\n",
        "# Metric definition for validation data\n",
        "def compute_metrics(eval_pred, num_labels=3):\n",
        "  if num_labels>1:  # for classification\n",
        "    metric = load(\"accuracy\")\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "  else:  # for regression\n",
        "    metric = load(\"spearmanr\")\n",
        "    predictions, labels = eval_pred\n",
        "\n",
        "  return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "\n",
        "# Define the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset_classification, # train_set, # train_dataset_classification,\n",
        "    eval_dataset=valid_dataset_classification, # valid_set, # valid_dataset_classification,\n",
        "    #data_collator=data_collator,  # the custom data collator\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate the model\n",
        "eval_results = trainer.evaluate()\n",
        "print(f\"Evaluation results: {eval_results}\")\n",
        "\n",
        "# Make predictions on new data\n",
        "predictions = trainer.predict(test_dataset)\n",
        "print(f\"Predictions: {predictions}\")"
      ],
      "metadata": {
        "id": "CDPjkP4OPwKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "AEadpbKlOqXG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "smICDQArpn-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sbk5zeVjpoCa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}